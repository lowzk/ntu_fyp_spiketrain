{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike train\n",
    "- Set hyperparameters\n",
    "- Load data\n",
    "- Generate spike train\n",
    "- Prune spike train\n",
    "- Train and test using various models\n",
    "\n",
    "Note: To test for memory and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently updating\n",
    "\n",
    "1. Record the saved file size and compare with the original representation\n",
    "2. Compare the number of MAC operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from spikenet import dataset, neuron\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DADx(adj, x, a=0.5, b=0.5):\n",
    "  degree = np.array(adj.sum(1)).flatten()\n",
    "  D_inv_a = np.power(degree, -a, where=degree!=0)\n",
    "  D_inv_b = np.power(degree, -b, where=degree!=0)\n",
    "  D_inv_a = sp.diags(D_inv_a)\n",
    "  D_inv_b = sp.diags(D_inv_b)\n",
    "  transformed_x = D_inv_a @ adj @ D_inv_b @ x\n",
    "  return torch.FloatTensor(transformed_x)\n",
    "\n",
    "def _generate_dynamic_spike_train(data: dataset.Dataset, hp: Dict, snn) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    data.adj : shape = (T, N, N) or list of length T of adjacency matrices\n",
    "    data.x   : shape = (T, N, F) or list of length T of node features\n",
    "    T = number of snapshots\n",
    "    N = number of nodes\n",
    "    F = number of node features\n",
    "\n",
    "    hp[\"time_steps\"] = how many internal SNN time steps to simulate per snapshot\n",
    "    hp[\"a\"], hp[\"b\"] = exponents in D^(-a) A D^(-b)\n",
    "    \"\"\"\n",
    "    spike_train_all = []\n",
    "    DADx_prev = None\n",
    "    spikes_prev = None\n",
    "    threshold = hp[\"threshold\"]\n",
    "\n",
    "    T = len(data.adj)       # number of snapshots\n",
    "    # 2) Loop over each snapshot\n",
    "    for t in range(T):\n",
    "        snn.reset()\n",
    "        adj_t = data.adj[t]  # NxN\n",
    "        x_t = data.x[t]      # NxF\n",
    "        DADx_t = get_DADx(adj_t, x_t, a=hp[\"a\"], b=hp[\"b\"])\n",
    "\n",
    "        if DADx_prev is not None:\n",
    "            # delta shape: (num_nodes,), mask shape: (num_nodes,)\n",
    "            delta = torch.abs(DADx_t - DADx_prev).max(dim=1)[0] # Get the max feature difference for each node\n",
    "            mask = delta < threshold # Mask nodes whose change is insignificant\n",
    "            DADx_t[mask] = 0\n",
    "\n",
    "        spike_trains_this_snapshot = []\n",
    "        for _ in range(hp[\"time_steps\"]):\n",
    "            spikes = snn(DADx_t)\n",
    "            spike_trains_this_snapshot.append(spikes)\n",
    "        spikes_t = torch.stack(spike_trains_this_snapshot)\n",
    "\n",
    "        if spikes_prev is not None:\n",
    "            for i in range(hp[\"time_steps\"]):\n",
    "                x = spikes_prev[i][mask]\n",
    "                print(f\"x shape: {x.shape} spikes brought over: {x.sum()}\")\n",
    "                spikes_t[i][mask] = spikes_prev[i][mask]\n",
    "\n",
    "        spike_train_all.append(spikes_t)\n",
    "        DADx_prev = DADx_t\n",
    "        spikes_prev = spikes_t\n",
    "\n",
    "    # 3) Concatenate all T snapshots if you’d like: shape = (T, time_steps, N, F)\n",
    "    spike_train_all = torch.stack(spike_train_all, dim=0)\n",
    "    spike_train_all = spike_train_all.view(-1, spike_train_all.size(-2), spike_train_all.size(-1))\n",
    "\n",
    "    # (Optional) convert to bool for memory savings\n",
    "    spike_train_all = spike_train_all.to(torch.bool)\n",
    "\n",
    "    return spike_train_all\n",
    "\n",
    "def _generate_static_spike_train(data: dataset.Dataset, hp: Dict, snn) -> torch.Tensor:\n",
    "  spike_train = []\n",
    "  DADx = get_DADx(data.adj[-1], data.x[-1], a=hp[\"a\"], b=hp[\"b\"])\n",
    "  for _ in range(hp[\"time_steps\"]):\n",
    "    spike_train.append(snn(DADx))\n",
    "  return torch.stack(spike_train).to(torch.bool)\n",
    "\n",
    "def generate_spike_train(data: dataset.Dataset, hp: Dict) -> torch.Tensor:\n",
    "  if hp[\"act\"] == \"IF\":\n",
    "      snn = neuron.IF(alpha=hp[\"alpha\"], surrogate=hp[\"surrogate\"])\n",
    "  elif hp[\"act\"] == \"LIF\":\n",
    "      snn = neuron.LIF(tau=hp[\"tau\"], alpha=hp[\"alpha\"], surrogate=hp[\"surrogate\"])\n",
    "  elif hp[\"act\"] == \"PLIF\":\n",
    "      snn = neuron.PLIF(tau=hp[\"tau\"], alpha=hp[\"alpha\"], surrogate=hp[\"surrogate\"])\n",
    "\n",
    "  if hp[\"graph_type\"]==\"static\":\n",
    "    return _generate_static_spike_train(data, hp, snn)\n",
    "  else:\n",
    "    # Final shape should be (time_steps, num_nodes, num_features)\n",
    "    return _generate_dynamic_spike_train(data, hp, snn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_spikes(spike_train, hp: Dict) -> torch.Tensor:\n",
    "  num_spikes = torch.sum(spike_train, dim=(1,2))\n",
    "  prune_param = hp[\"prune_param\"]\n",
    "  median = torch.median(num_spikes)\n",
    "  pruned_start_idx = 0\n",
    "  while(num_spikes[pruned_start_idx] < median * prune_param):\n",
    "    pruned_start_idx += 1\n",
    "  return spike_train[pruned_start_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML models to classify the spike train\n",
    "- LSTM\n",
    "- MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeTrainDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X  # Need to typecast back into a float later\n",
    "        self.y = y.long()   # Ensure labels are long tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "    super().__init__()\n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out, (hn, _) = self.lstm(x)\n",
    "    out = self.fc(hn[-1])\n",
    "    return out\n",
    "  \n",
    "class MLPClassifier(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_classes):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.fc1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hyperparameters(hyperparameters):\n",
    "  if hyperparameters['dataset'] not in ['DBLP',]:\n",
    "    raise Exception(\"Invalid dataset name\")\n",
    "  if (hyperparameters['a']+hyperparameters['b']!=1):\n",
    "    raise Exception(\"a+b must be equal to 1\")\n",
    "  if hyperparameters['a']<0 or hyperparameters['b']<0:\n",
    "    raise Exception(\"a and b must be positive\")\n",
    "  if hyperparameters[\"graph_type\"] not in [\"static\", \"dynamic\"]:\n",
    "    raise Exception(\"Invalid graph type, only static and dynamic are allowed\")\n",
    "  if hyperparameters[\"graph_type\"]==\"static\":\n",
    "    if hyperparameters[\"time_steps\"] is None:\n",
    "      raise Exception(\"time_steps is required for static graph\")\n",
    "  if hyperparameters[\"act\"] not in [\"IF\", \"LIF\", \"PLIF\"]:\n",
    "    raise Exception(\"Invalid activation function, only IF, LIF and PLIF are allowed\")\n",
    "  \n",
    "def print_confusion_matrix(all_labels, all_preds):\n",
    "  cm = confusion_matrix(all_labels, all_preds)\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "              xticklabels=range(10), yticklabels=range(10))\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('True')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()\n",
    "\n",
    "def get_tensor_memory(tensor):\n",
    "  element_size = tensor.element_size()  # Size of each element in bytes\n",
    "  num_elements = tensor.numel()         # Total number of elements\n",
    "  total_memory = element_size * num_elements      # Total memory in bytes\n",
    "  total_memory_mb = total_memory / (1024 ** 2)     # Convert to megabytes\n",
    "  return total_memory_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_tensor(tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Packs a tensor of 1s and 0s into a space-optimized representation.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): A float32 tensor containing 1s and 0s.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A packed tensor (torch.uint8) with 1 bit per element.\n",
    "        tuple: The original shape of the tensor for unpacking.\n",
    "    \"\"\"\n",
    "    # Ensure the tensor is a float and convert to boolean (0 -> False, 1 -> True)\n",
    "    tensor = tensor.to(torch.bool)\n",
    "    original_shape = tensor.shape\n",
    "    \n",
    "    # Flatten the tensor and convert to numpy for bit packing\n",
    "    flattened = tensor.flatten().numpy().astype(np.uint8)\n",
    "    packed = np.packbits(flattened)  # Packs 8 boolean values into 1 byte\n",
    "    \n",
    "    # Convert back to a torch tensor\n",
    "    packed_tensor = torch.from_numpy(packed).to(torch.uint8)\n",
    "    return packed_tensor, original_shape\n",
    "\n",
    "def unpack_tensor(packed: torch.Tensor, original_shape: tuple):\n",
    "    \"\"\"\n",
    "    Unpacks a packed tensor back into its original form.\n",
    "    \n",
    "    Args:\n",
    "        packed (torch.Tensor): A packed tensor (torch.uint8) with 1 bit per element.\n",
    "        original_shape (tuple): The original shape of the tensor.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The unpacked tensor.\n",
    "    \"\"\"\n",
    "    # Convert to numpy and unpack the bits\n",
    "    unpacked = np.unpackbits(packed.numpy())\n",
    "    \n",
    "    # Convert back to a torch tensor and reshape\n",
    "    unpacked_tensor = torch.from_numpy(unpacked).to(torch.float32)\n",
    "    unpacked_tensor = unpacked_tensor[:np.prod(original_shape)].reshape(original_shape)\n",
    "    return unpacked_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single input to output pipeline\n",
    "What to track\n",
    "- Memory usage for spike train\n",
    "- Memory usage in training model\n",
    "- Time taken to train model\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(hyperparameters, system_params):\n",
    "    check_hyperparameters(hyperparameters)\n",
    "\n",
    "    # ----------------------\n",
    "    # Data Loading and Setup\n",
    "    # ----------------------\n",
    "    if hyperparameters['dataset'] == 'DBLP':\n",
    "        data = dataset.DBLP()\n",
    "\n",
    "    original_memory = get_tensor_memory(data.x if hyperparameters[\"graph_type\"]==\"dynamic\" else data.x[-1])\n",
    "    original_num_elements = data.x.numel() if hyperparameters[\"graph_type\"]==\"dynamic\" else data.x[-1].numel()\n",
    "\n",
    "    spike_train = generate_spike_train(data, hyperparameters)\n",
    "\n",
    "    print(spike_train.shape)\n",
    "    \n",
    "    if hyperparameters[\"prune_param\"] is not None:\n",
    "        spike_train = prune_spikes(spike_train, hyperparameters)\n",
    "\n",
    "    # Permute to [samples, time, features] if RNN-based model\n",
    "    spike_train = spike_train.permute(1, 0, 2)\n",
    "\n",
    "    # For MLP, flatten the [time, features] dimension\n",
    "    if hyperparameters[\"model\"]==\"MLP\":\n",
    "        spike_train = spike_train.reshape(spike_train.shape[0], -1)  # [samples, time * features]\n",
    "\n",
    "    print(spike_train.shape)\n",
    "    # --------------\n",
    "    # Compression (packing) example\n",
    "    # --------------\n",
    "    compressed_spike_train, original_shape = pack_tensor(spike_train)\n",
    "    # Show theoretical space savings by \"packing\"\n",
    "    spike_train = unpack_tensor(compressed_spike_train, original_shape)\n",
    "    final_memory = get_tensor_memory(compressed_spike_train)\n",
    "\n",
    "    if system_params[\"save_tensor\"]:\n",
    "        if hyperparameters[\"graph_type\"]==\"dynamic\":\n",
    "            data.x.numpy().tofile(f\"{hyperparameters['dataset']}_x_original.npy\")\n",
    "        else:\n",
    "            data.x[-1].numpy().tofile(f\"{hyperparameters['dataset']}_x[-1]_original.npy\")\n",
    "        compressed_spike_train.numpy().tofile(f\"{hyperparameters['dataset']}_spike_train_compressed.npy\")\n",
    "\n",
    "    final_num_elements = spike_train.numel()\n",
    "    if system_params[\"test_memory\"]:\n",
    "        print(f\"Original memory: {original_memory:.2f} MB, Final memory: {final_memory:.2f} MB\")\n",
    "        print(f\"Original num elements: {original_num_elements}, Final num elements: {final_num_elements}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Train/Test Split\n",
    "    # -----------------------\n",
    "    y = data.y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(spike_train, y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y)\n",
    "\n",
    "    train_dataset = SpikeTrainDataset(X_train, y_train)\n",
    "    test_dataset  = SpikeTrainDataset(X_test,  y_test)\n",
    "\n",
    "    batch_size = system_params[\"batch_size\"]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ------------------------\n",
    "    # Model Definition\n",
    "    # ------------------------\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    if hyperparameters[\"model\"] == \"LSTM\":\n",
    "        # LSTM expects [batch_size, seq_len, input_size]\n",
    "        model = LSTMClassifier(input_size=spike_train.shape[-1],\n",
    "                               hidden_size=256,\n",
    "                               num_layers=2,\n",
    "                               num_classes=data.num_classes).to(device)\n",
    "    elif hyperparameters[\"model\"] == \"MLP\":\n",
    "        # MLP expects [batch_size, input_size]\n",
    "        model = MLPClassifier(input_size=spike_train.shape[-1],\n",
    "                              hidden_size=256,\n",
    "                              num_classes=data.num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = system_params[\"num_epochs\"]\n",
    "\n",
    "    # ---------------------------------\n",
    "    # Measure MACs for a Single Forward\n",
    "    # ---------------------------------\n",
    "    # We'll approximate training MACs as (forward MACs + backward MACs)\n",
    "    # Typically backward pass ~2x forward pass => total ~3x forward pass.\n",
    "    # Also measure inference MACs for the test set.\n",
    "\n",
    "    # Get a single batch from train_loader for MAC profiling\n",
    "    dummy_input, _ = next(iter(train_loader))\n",
    "    dummy_input = dummy_input.float().to(device)\n",
    "\n",
    "    # Profile the forward pass\n",
    "    macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "    print(f\"Single-batch MACs (forward): {macs:.2f}, Number of parameters: {params}\")\n",
    "\n",
    "    # Multiply by the number of training batches and epochs\n",
    "    macs_per_epoch_forward = macs * len(train_loader)\n",
    "    training_macs_forward = macs_per_epoch_forward * num_epochs\n",
    "\n",
    "    # Approximate backward pass cost as 2× forward\n",
    "    # (This is a rough rule of thumb, actual overhead can vary.)\n",
    "    training_macs_backward = 2 * training_macs_forward\n",
    "\n",
    "    # Total training MACs\n",
    "    total_training_macs = training_macs_forward + training_macs_backward\n",
    "    print(f\"Approx. total training MACs (forward+backward): {total_training_macs:.2f}\")\n",
    "\n",
    "    # Inference (test) MACs: #batches × single forward pass\n",
    "    inference_macs = macs * len(test_loader)\n",
    "    print(f\"Approx. total test inference MACs: {inference_macs:.2f}\")\n",
    "\n",
    "    # --------------\n",
    "    # Training Loop\n",
    "    # --------------\n",
    "    start_time = time.time()\n",
    "    final_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.float().to(device)  # Convert back to float\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100.0 * correct / total\n",
    "        final_accuracy = accuracy\n",
    "        if system_params[\"verbose\"]:\n",
    "            print(f'Accuracy on test set: {accuracy:.2f}%\\n')\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # ----------------------------\n",
    "    # Gather Predictions for CM\n",
    "    # ----------------------------\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    if system_params[\"verbose\"]:\n",
    "        print_confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return final_accuracy, time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_hyperparameters = {\n",
    "    \"dataset\": \"DBLP\", # DBLP\n",
    "    \"graph_type\": \"dynamic\", # static, dynamic\n",
    "    \"time_steps\": 20, # Required for static graph\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 1.0,\n",
    "    \"surrogate\": \"triangle\", \n",
    "    \"act\": \"LIF\", # IF, LIF, PLIF\n",
    "    \"a\": 0.5, # a+b=1\n",
    "    \"b\": 0.5, # a+b=1\n",
    "    \"prune_param\": None, # Float or None\n",
    "    \"model\": \"MLP\", # LSTM, MLP\n",
    "    \"threshold\": 0.5\n",
    "}\n",
    "\n",
    "baseline_hyperparameters_copy = baseline_hyperparameters.copy()\n",
    "\n",
    "system_params = {\n",
    "  \"batch_size\": 64,\n",
    "  \"num_epochs\": 20,\n",
    "  \"verbose\": False,\n",
    "  \"test_memory\": True,\n",
    "  \"save_tensor\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  acc, time_taken = get_results(baseline_hyperparameters, system_params)\n",
    "  print(f\"Accuracy: {acc:.2f}%, Time taken: {time_taken:.2f} seconds\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "- Static vs Dynamic graphs\n",
    "- MLP vs LSTM\n",
    "- tau values\n",
    "- Number of time steps for static graph\n",
    "- a and b values\n",
    "- Prune param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static and dynamic graphs for MLP and LSTM\n",
    "graph_types = [\"static\", \"dynamic\"]\n",
    "models = [\"MLP\", \"LSTM\"]\n",
    "acc_list = []\n",
    "time_list = []\n",
    "for graph_type in graph_types:\n",
    "  for model in models:\n",
    "    print(f\"Currently testing: Graph type: {graph_type}, Model: {model}\")\n",
    "    baseline_hyperparameters[\"graph_type\"] = graph_type\n",
    "    baseline_hyperparameters[\"model\"] = model\n",
    "    acc, time_taken = get_results(baseline_hyperparameters, system_params)\n",
    "    acc_list.append(acc)\n",
    "    time_list.append(time_taken)\n",
    "\n",
    "idx = 0\n",
    "for graph_type in graph_types:\n",
    "  for model in models:\n",
    "    print(f\"Graph type: {graph_type}, Model: {model}, Accuracy: {acc_list[idx]:.2f}%, Time: {time_list[idx]:.2f} seconds\")\n",
    "    idx += 1\n",
    "\n",
    "baseline_hyperparameters = baseline_hyperparameters_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps and tau\n",
    "time_steps_list = [10, 20, 30, 40, 50, 60]\n",
    "tau_list = [1.0, 2.0, 5.0, 10.0]\n",
    "acc_list = []\n",
    "time_list = []\n",
    "\n",
    "for time_steps in time_steps_list:\n",
    "  for tau in tau_list:\n",
    "    print(f\"Currently testing: Time steps: {time_steps}, Tau: {tau}\")\n",
    "    baseline_hyperparameters[\"time_steps\"] = time_steps\n",
    "    baseline_hyperparameters[\"tau\"] = tau\n",
    "    acc, time_taken = get_results(baseline_hyperparameters, system_params)\n",
    "    acc_list.append(acc)\n",
    "    time_list.append(time_taken)\n",
    "\n",
    "idx = 0\n",
    "for time_steps in time_steps_list:\n",
    "  for tau in tau_list:\n",
    "    print(f\"Time steps: {time_steps}, Tau: {tau}, Accuracy: {acc_list[idx]:.2f}%, Time: {time_list[idx]:.2f} seconds\")\n",
    "    idx += 1\n",
    "\n",
    "baseline_hyperparameters = baseline_hyperparameters_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a and b values and prune param\n",
    "a_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "prune_list = [None, 0.6, 0.8, 1.0]\n",
    "acc_list = []\n",
    "time_list = []\n",
    "\n",
    "for a in a_list:\n",
    "  for prune in prune_list:\n",
    "    print(f\"Currently testing: a: {a}, b: {1-a}, Prune: {prune}\")\n",
    "    baseline_hyperparameters[\"a\"] = a\n",
    "    baseline_hyperparameters[\"b\"] = 1-a\n",
    "    baseline_hyperparameters[\"prune_param\"] = prune\n",
    "    acc, time_taken = get_results(baseline_hyperparameters, system_params)\n",
    "    acc_list.append(acc)\n",
    "    time_list.append(time_taken)\n",
    "\n",
    "idx = 0\n",
    "for a in a_list:\n",
    "  for prune in prune_list:\n",
    "    print(f\"a: {a}, b: {1-a}, Prune: {prune}, Accuracy: {acc_list[idx]:.2f}%, Time: {time_list[idx]:.2f} seconds\")\n",
    "    idx += 1\n",
    "\n",
    "baseline_hyperparameters = baseline_hyperparameters_copy.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the accuracy with time stamp relationship\n",
    "\n",
    "- Generate spike train (of length 10) for each time stamp\n",
    "- Train a separate MLP on each time step and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "    \"dataset\": \"DBLP\", # DBLP\n",
    "    \"graph_type\": \"dynamic\", # static, dynamic\n",
    "    \"time_steps\": 10, # Required for static graph\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 1.0,\n",
    "    \"surrogate\": \"triangle\", \n",
    "    \"act\": \"LIF\", # IF, LIF, PLIF\n",
    "    \"a\": 0.5, # a+b=1\n",
    "    \"b\": 0.5, # a+b=1\n",
    "    \"prune_param\": None, # Float or None\n",
    "    \"model\": \"LSTM\" # LSTM, MLP\n",
    "}\n",
    "\n",
    "system_params = {\n",
    "  \"batch_size\": 64,\n",
    "  \"num_epochs\": 20,\n",
    "  \"verbose\": False,\n",
    "  \"test_memory\": True,\n",
    "  \"save_tensor\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.DBLP()\n",
    "snn = neuron.LIF(tau=hp[\"tau\"], alpha=hp[\"alpha\"], surrogate=hp[\"surrogate\"])\n",
    "spike_train_all = []\n",
    "\n",
    "num_time_stamps = len(data.adj) # number of snapshots\n",
    "\n",
    "# Generate static spike train from the last snapshot\n",
    "spike_train = generate_spike_train(data, hp)\n",
    "print(spike_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_train = spike_train.view(num_time_stamps, hp[\"time_steps\"], spike_train.size(-2), spike_train.size(-1))\n",
    "print(spike_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a MLP on the different time stamps to see the accuracy over time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "y = data.y\n",
    "acc_list = []\n",
    "\n",
    "for t in range(num_time_stamps):\n",
    "  cur_spike_train = spike_train[t]\n",
    "  cur_spike_train = cur_spike_train.permute(1, 0, 2)\n",
    "  cur_spike_train = cur_spike_train.reshape(cur_spike_train.shape[0], -1)  # [samples, time * features]\n",
    "\n",
    "  y = data.y\n",
    "  X_train, X_test, y_train, y_test = train_test_split(cur_spike_train, y,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=42,\n",
    "                                                      stratify=y)\n",
    "\n",
    "  train_dataset = SpikeTrainDataset(X_train, y_train)\n",
    "  test_dataset  = SpikeTrainDataset(X_test,  y_test)\n",
    "\n",
    "  batch_size = system_params[\"batch_size\"]\n",
    "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  print(f'Using device: {device}')\n",
    "\n",
    "  model = MLPClassifier(input_size=cur_spike_train.shape[-1],\n",
    "                        hidden_size=256,\n",
    "                        num_classes=data.num_classes).to(device)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "  num_epochs = system_params[\"num_epochs\"]\n",
    "\n",
    "  start_time = time.time()\n",
    "  final_accuracy = 0\n",
    "  for epoch in range(num_epochs):\n",
    "      model.train()\n",
    "      running_loss = 0.0\n",
    "      for inputs, labels in train_loader:\n",
    "          inputs = inputs.float().to(device)  # Convert back to float\n",
    "          labels = labels.to(device)\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          outputs = model(inputs)\n",
    "          loss = criterion(outputs, labels)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          running_loss += loss.item()\n",
    "\n",
    "      avg_loss = running_loss / len(train_loader)\n",
    "      print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "      # Evaluation\n",
    "      model.eval()\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      with torch.no_grad():\n",
    "          for inputs, labels in test_loader:\n",
    "              inputs = inputs.float().to(device)\n",
    "              labels = labels.to(device)\n",
    "              outputs = model(inputs)\n",
    "              _, predicted = torch.max(outputs.data, 1)\n",
    "              total += labels.size(0)\n",
    "              correct += (predicted == labels).sum().item()\n",
    "\n",
    "      accuracy = 100.0 * correct / total\n",
    "      final_accuracy = accuracy\n",
    "      if system_params[\"verbose\"]:\n",
    "          print(f'Accuracy on test set: {accuracy:.2f}%\\n')\n",
    "\n",
    "  end_time = time.time()\n",
    "  time_taken = end_time - start_time\n",
    "  acc_list.append(final_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot T against time_stamp where the y axis ranges from 0 to 100\n",
    "# Make sure the y axis starts from 0 and ends at 100\n",
    "plt.plot(range(num_time_stamps), acc_list)\n",
    "plt.xlabel(\"Time Stamp\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy over Time\")\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
