{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike train\n",
    "- Set hyperparameters\n",
    "- Load data\n",
    "- Generate spike train\n",
    "- Prune spike train\n",
    "- Train and test using various models\n",
    "\n",
    "Note: To test for memory and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently updating\n",
    "\n",
    "1. Record the saved file size and compare with the original representation\n",
    "2. Compare the number of MAC operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from spikenet import dataset, neuron\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DADx(adj, x, a=0.5, b=0.5):\n",
    "  degree = np.array(adj.sum(1)).flatten()\n",
    "  D_inv_a = np.power(degree, -a, where=degree!=0)\n",
    "  D_inv_b = np.power(degree, -b, where=degree!=0)\n",
    "  D_inv_a = sp.diags(D_inv_a)\n",
    "  D_inv_b = sp.diags(D_inv_b)\n",
    "  transformed_x = D_inv_a @ adj @ D_inv_b @ x\n",
    "  return torch.FloatTensor(transformed_x)\n",
    "\n",
    "def generate_spike_train(data: dataset.Dataset, hp: Dict) -> torch.Tensor:\n",
    "  spike_train = []\n",
    "  if hp[\"act\"]==\"IF\":\n",
    "    snn = neuron.IF(alpha=hp[\"alpha\"], surrogate=hp[\"surrogate\"])\n",
    "  elif hp[\"act\"]==\"LIF\":\n",
    "    snn = neuron.LIF(tau=hp[\"tau\"], alpha=hp[\"alpha\"], surrogate=hp[\"surrogate\"])\n",
    "  elif hp[\"act\"]==\"PLIF\":\n",
    "    snn = neuron.PLIF(tau=hp[\"tau\"], alpha=hp[\"alpha\"], surrogate=hp[\"surrogate\"])\n",
    "  \n",
    "  if hp[\"graph_type\"]==\"static\":\n",
    "    DADx = get_DADx(data.adj[-1], data.x[-1], a=hp[\"a\"], b=hp[\"b\"])\n",
    "    for _ in range(hp[\"time_steps\"]):\n",
    "      spike_train.append(snn(DADx))\n",
    "  else:\n",
    "    for adj, x in zip(data.adj, data.x):\n",
    "      spike_train.append(snn(get_DADx(adj, x, a=hp[\"a\"], b=hp[\"b\"])))\n",
    "  return torch.stack(spike_train).to(torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_spikes(spike_train, hp: Dict) -> torch.Tensor:\n",
    "  num_spikes = torch.sum(spike_train, dim=(1,2))\n",
    "  prune_param = hp[\"prune_param\"]\n",
    "  median = torch.median(num_spikes)\n",
    "  pruned_start_idx = 0\n",
    "  while(num_spikes[pruned_start_idx] < median * prune_param):\n",
    "    pruned_start_idx += 1\n",
    "  return spike_train[pruned_start_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML models to classify the spike train\n",
    "- LSTM\n",
    "- MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeTrainDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X  # Need to typecast back into a float later\n",
    "        self.y = y.long()   # Ensure labels are long tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "    super().__init__()\n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out, (hn, _) = self.lstm(x)\n",
    "    out = self.fc(hn[-1])\n",
    "    return out\n",
    "  \n",
    "class MLPClassifier(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_classes):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.fc1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_hyperparameters(hyperparameters):\n",
    "  if hyperparameters['dataset'] not in ['DBLP',]:\n",
    "    raise Exception(\"Invalid dataset name\")\n",
    "  if (hyperparameters['a']+hyperparameters['b']!=1):\n",
    "    raise Exception(\"a+b must be equal to 1\")\n",
    "  if hyperparameters['a']<0 or hyperparameters['b']<0:\n",
    "    raise Exception(\"a and b must be positive\")\n",
    "  if hyperparameters[\"graph_type\"] not in [\"static\", \"dynamic\"]:\n",
    "    raise Exception(\"Invalid graph type, only static and dynamic are allowed\")\n",
    "  if hyperparameters[\"graph_type\"]==\"static\":\n",
    "    if hyperparameters[\"time_steps\"] is None:\n",
    "      raise Exception(\"time_steps is required for static graph\")\n",
    "  if hyperparameters[\"act\"] not in [\"IF\", \"LIF\", \"PLIF\"]:\n",
    "    raise Exception(\"Invalid activation function, only IF, LIF and PLIF are allowed\")\n",
    "  \n",
    "def print_confusion_matrix(all_labels, all_preds):\n",
    "  cm = confusion_matrix(all_labels, all_preds)\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "              xticklabels=range(10), yticklabels=range(10))\n",
    "  plt.xlabel('Predicted')\n",
    "  plt.ylabel('True')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()\n",
    "\n",
    "def get_tensor_memory(tensor):\n",
    "  element_size = tensor.element_size()  # Size of each element in bytes\n",
    "  num_elements = tensor.numel()         # Total number of elements\n",
    "  total_memory = element_size * num_elements      # Total memory in bytes\n",
    "  total_memory_mb = total_memory / (1024 ** 2)     # Convert to megabytes\n",
    "  return total_memory_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_tensor(tensor: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Packs a tensor of 1s and 0s into a space-optimized representation.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): A float32 tensor containing 1s and 0s.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A packed tensor (torch.uint8) with 1 bit per element.\n",
    "        tuple: The original shape of the tensor for unpacking.\n",
    "    \"\"\"\n",
    "    # Ensure the tensor is a float and convert to boolean (0 -> False, 1 -> True)\n",
    "    tensor = tensor.to(torch.bool)\n",
    "    original_shape = tensor.shape\n",
    "    \n",
    "    # Flatten the tensor and convert to numpy for bit packing\n",
    "    flattened = tensor.flatten().numpy().astype(np.uint8)\n",
    "    packed = np.packbits(flattened)  # Packs 8 boolean values into 1 byte\n",
    "    \n",
    "    # Convert back to a torch tensor\n",
    "    packed_tensor = torch.from_numpy(packed).to(torch.uint8)\n",
    "    return packed_tensor, original_shape\n",
    "\n",
    "def unpack_tensor(packed: torch.Tensor, original_shape: tuple):\n",
    "    \"\"\"\n",
    "    Unpacks a packed tensor back into its original form.\n",
    "    \n",
    "    Args:\n",
    "        packed (torch.Tensor): A packed tensor (torch.uint8) with 1 bit per element.\n",
    "        original_shape (tuple): The original shape of the tensor.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The unpacked tensor.\n",
    "    \"\"\"\n",
    "    # Convert to numpy and unpack the bits\n",
    "    unpacked = np.unpackbits(packed.numpy())\n",
    "    \n",
    "    # Convert back to a torch tensor and reshape\n",
    "    unpacked_tensor = torch.from_numpy(unpacked).to(torch.float32)\n",
    "    unpacked_tensor = unpacked_tensor[:np.prod(original_shape)].reshape(original_shape)\n",
    "    return unpacked_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single input to output pipeline\n",
    "What to track\n",
    "- Memory usage for spike train\n",
    "- Memory usage in training model\n",
    "- Time taken to train model\n",
    "- Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(hyperparameters, system_params):\n",
    "    check_hyperparameters(hyperparameters)\n",
    "\n",
    "    # ----------------------\n",
    "    # Data Loading and Setup\n",
    "    # ----------------------\n",
    "    if hyperparameters['dataset'] == 'DBLP':\n",
    "        data = dataset.DBLP()\n",
    "\n",
    "    original_memory = get_tensor_memory(data.x if hyperparameters[\"graph_type\"]==\"dynamic\" else data.x[-1])\n",
    "    original_num_elements = data.x.numel() if hyperparameters[\"graph_type\"]==\"dynamic\" else data.x[-1].numel()\n",
    "\n",
    "    spike_train = generate_spike_train(data, hyperparameters)\n",
    "    if hyperparameters[\"prune_param\"] is not None:\n",
    "        spike_train = prune_spikes(spike_train, hyperparameters)\n",
    "\n",
    "    # Permute to [samples, time, features] if RNN-based model\n",
    "    spike_train = spike_train.permute(1, 0, 2)\n",
    "\n",
    "    # For MLP, flatten the [time, features] dimension\n",
    "    if hyperparameters[\"model\"]==\"MLP\":\n",
    "        spike_train = spike_train.reshape(spike_train.shape[0], -1)  # [samples, time * features]\n",
    "\n",
    "    # --------------\n",
    "    # Compression (packing) example\n",
    "    # --------------\n",
    "    compressed_spike_train, original_shape = pack_tensor(spike_train)\n",
    "    # Show theoretical space savings by \"packing\"\n",
    "    spike_train = unpack_tensor(compressed_spike_train, original_shape)\n",
    "    final_memory = get_tensor_memory(compressed_spike_train)\n",
    "\n",
    "    if system_params[\"save_tensor\"]:\n",
    "        if hyperparameters[\"graph_type\"]==\"dynamic\":\n",
    "            data.x.numpy().tofile(f\"{hyperparameters['dataset']}_x_original.npy\")\n",
    "        else:\n",
    "            data.x[-1].numpy().tofile(f\"{hyperparameters['dataset']}_x[-1]_original.npy\")\n",
    "        compressed_spike_train.numpy().tofile(f\"{hyperparameters['dataset']}_spike_train_compressed.npy\")\n",
    "\n",
    "    final_num_elements = spike_train.numel()\n",
    "    if system_params[\"test_memory\"]:\n",
    "        print(f\"Original memory: {original_memory:.2f} MB, Final memory: {final_memory:.2f} MB\")\n",
    "        print(f\"Original num elements: {original_num_elements}, Final num elements: {final_num_elements}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Train/Test Split\n",
    "    # -----------------------\n",
    "    y = data.y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(spike_train, y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=y)\n",
    "\n",
    "    train_dataset = SpikeTrainDataset(X_train, y_train)\n",
    "    test_dataset  = SpikeTrainDataset(X_test,  y_test)\n",
    "\n",
    "    batch_size = system_params[\"batch_size\"]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ------------------------\n",
    "    # Model Definition\n",
    "    # ------------------------\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    if hyperparameters[\"model\"] == \"LSTM\":\n",
    "        # LSTM expects [batch_size, seq_len, input_size]\n",
    "        model = LSTMClassifier(input_size=spike_train.shape[-1],\n",
    "                               hidden_size=256,\n",
    "                               num_layers=2,\n",
    "                               num_classes=data.num_classes).to(device)\n",
    "    elif hyperparameters[\"model\"] == \"MLP\":\n",
    "        # MLP expects [batch_size, input_size]\n",
    "        model = MLPClassifier(input_size=spike_train.shape[-1],\n",
    "                              hidden_size=256,\n",
    "                              num_classes=data.num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = system_params[\"num_epochs\"]\n",
    "\n",
    "    # ---------------------------------\n",
    "    # Measure MACs for a Single Forward\n",
    "    # ---------------------------------\n",
    "    # We'll approximate training MACs as (forward MACs + backward MACs)\n",
    "    # Typically backward pass ~2x forward pass => total ~3x forward pass.\n",
    "    # Also measure inference MACs for the test set.\n",
    "\n",
    "    # Get a single batch from train_loader for MAC profiling\n",
    "    dummy_input, _ = next(iter(train_loader))\n",
    "    dummy_input = dummy_input.float().to(device)\n",
    "\n",
    "    # Profile the forward pass\n",
    "    macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "    print(f\"Single-batch MACs (forward): {macs:.2f}, Number of parameters: {params}\")\n",
    "\n",
    "    # Multiply by the number of training batches and epochs\n",
    "    macs_per_epoch_forward = macs * len(train_loader)\n",
    "    training_macs_forward = macs_per_epoch_forward * num_epochs\n",
    "\n",
    "    # Approximate backward pass cost as 2× forward\n",
    "    # (This is a rough rule of thumb, actual overhead can vary.)\n",
    "    training_macs_backward = 2 * training_macs_forward\n",
    "\n",
    "    # Total training MACs\n",
    "    total_training_macs = training_macs_forward + training_macs_backward\n",
    "    print(f\"Approx. total training MACs (forward+backward): {total_training_macs:.2f}\")\n",
    "\n",
    "    # Inference (test) MACs: #batches × single forward pass\n",
    "    inference_macs = macs * len(test_loader)\n",
    "    print(f\"Approx. total test inference MACs: {inference_macs:.2f}\")\n",
    "\n",
    "    # --------------\n",
    "    # Training Loop\n",
    "    # --------------\n",
    "    start_time = time.time()\n",
    "    final_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.float().to(device)  # Convert back to float\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.float().to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100.0 * correct / total\n",
    "        final_accuracy = accuracy\n",
    "        if system_params[\"verbose\"]:\n",
    "            print(f'Accuracy on test set: {accuracy:.2f}%\\n')\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # ----------------------------\n",
    "    # Gather Predictions for CM\n",
    "    # ----------------------------\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    if system_params[\"verbose\"]:\n",
    "        print_confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    return final_accuracy, time_taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_hyperparameters = {\n",
    "    \"dataset\": \"DBLP\", # DBLP\n",
    "    \"graph_type\": \"static\", # static, dynamic\n",
    "    \"time_steps\": 20, # Required for static graph\n",
    "    \"tau\": 1.0,\n",
    "    \"alpha\": 1.0,\n",
    "    \"surrogate\": \"triangle\", \n",
    "    \"act\": \"LIF\", # IF, LIF, PLIF\n",
    "    \"a\": 0.5, # a+b=1\n",
    "    \"b\": 0.5, # a+b=1\n",
    "    \"prune_param\": None, # Float or None\n",
    "    \"model\": \"MLP\" # LSTM, MLP\n",
    "}\n",
    "\n",
    "baseline_hyperparameters_copy = baseline_hyperparameters.copy()\n",
    "\n",
    "system_params = {\n",
    "  \"batch_size\": 64,\n",
    "  \"num_epochs\": 20,\n",
    "  \"verbose\": False,\n",
    "  \"test_memory\": True,\n",
    "  \"save_tensor\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Single-batch MACs (forward): 42106880.00, Number of parameters: 658186.0\n",
      "Approx. total training MACs (forward+backward): 889297305600.00\n",
      "Approx. total test inference MACs: 3705405440.00\n",
      "Epoch [1/20], Loss: 1.0056\n",
      "Epoch [2/20], Loss: 0.8319\n",
      "Epoch [3/20], Loss: 0.7307\n",
      "Epoch [4/20], Loss: 0.6432\n",
      "Epoch [5/20], Loss: 0.5501\n",
      "Epoch [6/20], Loss: 0.4670\n",
      "Epoch [7/20], Loss: 0.3876\n",
      "Epoch [8/20], Loss: 0.3135\n",
      "Epoch [9/20], Loss: 0.2464\n",
      "Epoch [10/20], Loss: 0.1874\n",
      "Epoch [11/20], Loss: 0.1459\n",
      "Epoch [12/20], Loss: 0.1041\n",
      "Epoch [13/20], Loss: 0.0769\n",
      "Epoch [14/20], Loss: 0.0555\n",
      "Epoch [15/20], Loss: 0.0500\n",
      "Epoch [16/20], Loss: 0.0604\n",
      "Epoch [17/20], Loss: 0.0399\n",
      "Epoch [18/20], Loss: 0.0437\n",
      "Epoch [19/20], Loss: 0.0563\n",
      "Epoch [20/20], Loss: 0.0577\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "  acc, time_taken = get_results(baseline_hyperparameters, system_params)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "- Static vs Dynamic graphs\n",
    "- MLP vs LSTM\n",
    "- tau values\n",
    "- Number of time steps for static graph\n",
    "- a and b values\n",
    "- Prune param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing: Graph type: static, Model: MLP\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0042\n",
      "Epoch [2/20], Loss: 0.8261\n",
      "Epoch [3/20], Loss: 0.7277\n",
      "Epoch [4/20], Loss: 0.6348\n",
      "Epoch [5/20], Loss: 0.5368\n",
      "Epoch [6/20], Loss: 0.4506\n",
      "Epoch [7/20], Loss: 0.3657\n",
      "Epoch [8/20], Loss: 0.2957\n",
      "Epoch [9/20], Loss: 0.2374\n",
      "Epoch [10/20], Loss: 0.1683\n",
      "Epoch [11/20], Loss: 0.1284\n",
      "Epoch [12/20], Loss: 0.1033\n",
      "Epoch [13/20], Loss: 0.0650\n",
      "Epoch [14/20], Loss: 0.0616\n",
      "Epoch [15/20], Loss: 0.0549\n",
      "Epoch [16/20], Loss: 0.0426\n",
      "Epoch [17/20], Loss: 0.0331\n",
      "Epoch [18/20], Loss: 0.0609\n",
      "Epoch [19/20], Loss: 0.0598\n",
      "Epoch [20/20], Loss: 0.0313\n",
      "Currently testing: Graph type: static, Model: LSTM\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0966\n",
      "Epoch [2/20], Loss: 0.8997\n",
      "Epoch [3/20], Loss: 0.8320\n",
      "Epoch [4/20], Loss: 0.7802\n",
      "Epoch [5/20], Loss: 0.7271\n",
      "Epoch [6/20], Loss: 0.6787\n",
      "Epoch [7/20], Loss: 0.6357\n",
      "Epoch [8/20], Loss: 0.5941\n",
      "Epoch [9/20], Loss: 0.5569\n",
      "Epoch [10/20], Loss: 0.5313\n",
      "Epoch [11/20], Loss: 0.4968\n",
      "Epoch [12/20], Loss: 0.4580\n",
      "Epoch [13/20], Loss: 0.4321\n",
      "Epoch [14/20], Loss: 0.3979\n",
      "Epoch [15/20], Loss: 0.3765\n",
      "Epoch [16/20], Loss: 0.3467\n",
      "Epoch [17/20], Loss: 0.3226\n",
      "Epoch [18/20], Loss: 0.3018\n",
      "Epoch [19/20], Loss: 0.2802\n",
      "Epoch [20/20], Loss: 0.2815\n",
      "Currently testing: Graph type: dynamic, Model: MLP\n",
      "Original memory: 370.26 MB, Final memory: 11.57 MB\n",
      "Original num elements: 97061760, Final num elements: 97061760\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0946\n",
      "Epoch [2/20], Loss: 0.8453\n",
      "Epoch [3/20], Loss: 0.7409\n",
      "Epoch [4/20], Loss: 0.6604\n",
      "Epoch [5/20], Loss: 0.5881\n",
      "Epoch [6/20], Loss: 0.5207\n",
      "Epoch [7/20], Loss: 0.4555\n",
      "Epoch [8/20], Loss: 0.3934\n",
      "Epoch [9/20], Loss: 0.3426\n",
      "Epoch [10/20], Loss: 0.3014\n",
      "Epoch [11/20], Loss: 0.2717\n",
      "Epoch [12/20], Loss: 0.2371\n",
      "Epoch [13/20], Loss: 0.2065\n",
      "Epoch [14/20], Loss: 0.1759\n",
      "Epoch [15/20], Loss: 0.1656\n",
      "Epoch [16/20], Loss: 0.1505\n",
      "Epoch [17/20], Loss: 0.1519\n",
      "Epoch [18/20], Loss: 0.1156\n",
      "Epoch [19/20], Loss: 0.1122\n",
      "Epoch [20/20], Loss: 0.1102\n",
      "Currently testing: Graph type: dynamic, Model: LSTM\n",
      "Original memory: 370.26 MB, Final memory: 11.57 MB\n",
      "Original num elements: 97061760, Final num elements: 97061760\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.1737\n",
      "Epoch [2/20], Loss: 0.9275\n",
      "Epoch [3/20], Loss: 0.8437\n",
      "Epoch [4/20], Loss: 0.7726\n",
      "Epoch [5/20], Loss: 0.7000\n",
      "Epoch [6/20], Loss: 0.6303\n",
      "Epoch [7/20], Loss: 0.5571\n",
      "Epoch [8/20], Loss: 0.4806\n",
      "Epoch [9/20], Loss: 0.4047\n",
      "Epoch [10/20], Loss: 0.3221\n",
      "Epoch [11/20], Loss: 0.2494\n",
      "Epoch [12/20], Loss: 0.1958\n",
      "Epoch [13/20], Loss: 0.1483\n",
      "Epoch [14/20], Loss: 0.1177\n",
      "Epoch [15/20], Loss: 0.0944\n",
      "Epoch [16/20], Loss: 0.0782\n",
      "Epoch [17/20], Loss: 0.0739\n",
      "Epoch [18/20], Loss: 0.0984\n",
      "Epoch [19/20], Loss: 0.0558\n",
      "Epoch [20/20], Loss: 0.0624\n",
      "Graph type: static, Model: MLP, Accuracy: 72.89%, Time: 10.00 seconds\n",
      "Graph type: static, Model: LSTM, Accuracy: 75.16%, Time: 194.71 seconds\n",
      "Graph type: dynamic, Model: MLP, Accuracy: 69.98%, Time: 11.62 seconds\n",
      "Graph type: dynamic, Model: LSTM, Accuracy: 72.37%, Time: 255.37 seconds\n"
     ]
    }
   ],
   "source": [
    "# Static and dynamic graphs for MLP and LSTM\n",
    "graph_types = [\"static\", \"dynamic\"]\n",
    "models = [\"MLP\", \"LSTM\"]\n",
    "acc_list = []\n",
    "time_list = []\n",
    "for graph_type in graph_types:\n",
    "  for model in models:\n",
    "    print(f\"Currently testing: Graph type: {graph_type}, Model: {model}\")\n",
    "    baseline_hyperparameters[\"graph_type\"] = graph_type\n",
    "    baseline_hyperparameters[\"model\"] = model\n",
    "    acc, time_taken = get_results(baseline_hyperparameters, system_params)\n",
    "    acc_list.append(acc)\n",
    "    time_list.append(time_taken)\n",
    "\n",
    "idx = 0\n",
    "for graph_type in graph_types:\n",
    "  for model in models:\n",
    "    print(f\"Graph type: {graph_type}, Model: {model}, Accuracy: {acc_list[idx]:.2f}%, Time: {time_list[idx]:.2f} seconds\")\n",
    "    idx += 1\n",
    "\n",
    "baseline_hyperparameters = baseline_hyperparameters_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing: Time steps: 10, Tau: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 4.29 MB\n",
      "Original num elements: 3594880, Final num elements: 35948800\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0204\n",
      "Epoch [2/20], Loss: 0.8342\n",
      "Epoch [3/20], Loss: 0.7319\n",
      "Epoch [4/20], Loss: 0.6447\n",
      "Epoch [5/20], Loss: 0.5725\n",
      "Epoch [6/20], Loss: 0.4926\n",
      "Epoch [7/20], Loss: 0.4122\n",
      "Epoch [8/20], Loss: 0.3515\n",
      "Epoch [9/20], Loss: 0.2847\n",
      "Epoch [10/20], Loss: 0.2229\n",
      "Epoch [11/20], Loss: 0.1770\n",
      "Epoch [12/20], Loss: 0.1302\n",
      "Epoch [13/20], Loss: 0.1049\n",
      "Epoch [14/20], Loss: 0.0718\n",
      "Epoch [15/20], Loss: 0.0599\n",
      "Epoch [16/20], Loss: 0.0487\n",
      "Epoch [17/20], Loss: 0.0381\n",
      "Epoch [18/20], Loss: 0.0335\n",
      "Epoch [19/20], Loss: 0.0482\n",
      "Epoch [20/20], Loss: 0.0742\n",
      "Currently testing: Time steps: 10, Tau: 2.0\n",
      "Original memory: 13.71 MB, Final memory: 4.29 MB\n",
      "Original num elements: 3594880, Final num elements: 35948800\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0226\n",
      "Epoch [2/20], Loss: 0.8106\n",
      "Epoch [3/20], Loss: 0.7027\n",
      "Epoch [4/20], Loss: 0.5997\n",
      "Epoch [5/20], Loss: 0.5075\n",
      "Epoch [6/20], Loss: 0.4077\n",
      "Epoch [7/20], Loss: 0.3130\n",
      "Epoch [8/20], Loss: 0.2318\n",
      "Epoch [9/20], Loss: 0.1683\n",
      "Epoch [10/20], Loss: 0.1223\n",
      "Epoch [11/20], Loss: 0.0753\n",
      "Epoch [12/20], Loss: 0.0460\n",
      "Epoch [13/20], Loss: 0.0326\n",
      "Epoch [14/20], Loss: 0.0267\n",
      "Epoch [15/20], Loss: 0.0247\n",
      "Epoch [16/20], Loss: 0.0441\n",
      "Epoch [17/20], Loss: 0.0303\n",
      "Epoch [18/20], Loss: 0.0218\n",
      "Epoch [19/20], Loss: 0.0204\n",
      "Epoch [20/20], Loss: 0.0163\n",
      "Currently testing: Time steps: 10, Tau: 5.0\n",
      "Original memory: 13.71 MB, Final memory: 4.29 MB\n",
      "Original num elements: 3594880, Final num elements: 35948800\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0459\n",
      "Epoch [2/20], Loss: 0.8005\n",
      "Epoch [3/20], Loss: 0.6874\n",
      "Epoch [4/20], Loss: 0.5859\n",
      "Epoch [5/20], Loss: 0.4712\n",
      "Epoch [6/20], Loss: 0.3639\n",
      "Epoch [7/20], Loss: 0.2601\n",
      "Epoch [8/20], Loss: 0.1708\n",
      "Epoch [9/20], Loss: 0.1077\n",
      "Epoch [10/20], Loss: 0.0669\n",
      "Epoch [11/20], Loss: 0.0442\n",
      "Epoch [12/20], Loss: 0.0326\n",
      "Epoch [13/20], Loss: 0.0272\n",
      "Epoch [14/20], Loss: 0.0275\n",
      "Epoch [15/20], Loss: 0.0222\n",
      "Epoch [16/20], Loss: 0.0234\n",
      "Epoch [17/20], Loss: 0.0160\n",
      "Epoch [18/20], Loss: 0.0188\n",
      "Epoch [19/20], Loss: 0.0191\n",
      "Epoch [20/20], Loss: 0.0293\n",
      "Currently testing: Time steps: 10, Tau: 10.0\n",
      "Original memory: 13.71 MB, Final memory: 4.29 MB\n",
      "Original num elements: 3594880, Final num elements: 35948800\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0673\n",
      "Epoch [2/20], Loss: 0.8060\n",
      "Epoch [3/20], Loss: 0.6925\n",
      "Epoch [4/20], Loss: 0.5799\n",
      "Epoch [5/20], Loss: 0.4698\n",
      "Epoch [6/20], Loss: 0.3603\n",
      "Epoch [7/20], Loss: 0.2599\n",
      "Epoch [8/20], Loss: 0.1789\n",
      "Epoch [9/20], Loss: 0.1196\n",
      "Epoch [10/20], Loss: 0.0695\n",
      "Epoch [11/20], Loss: 0.0465\n",
      "Epoch [12/20], Loss: 0.0371\n",
      "Epoch [13/20], Loss: 0.0315\n",
      "Epoch [14/20], Loss: 0.0269\n",
      "Epoch [15/20], Loss: 0.0252\n",
      "Epoch [16/20], Loss: 0.0205\n",
      "Epoch [17/20], Loss: 0.0196\n",
      "Epoch [18/20], Loss: 0.0197\n",
      "Epoch [19/20], Loss: 0.0202\n",
      "Epoch [20/20], Loss: 0.0226\n",
      "Currently testing: Time steps: 20, Tau: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0053\n",
      "Epoch [2/20], Loss: 0.8250\n",
      "Epoch [3/20], Loss: 0.7163\n",
      "Epoch [4/20], Loss: 0.6169\n",
      "Epoch [5/20], Loss: 0.5231\n",
      "Epoch [6/20], Loss: 0.4363\n",
      "Epoch [7/20], Loss: 0.3526\n",
      "Epoch [8/20], Loss: 0.2815\n",
      "Epoch [9/20], Loss: 0.2156\n",
      "Epoch [10/20], Loss: 0.1603\n",
      "Epoch [11/20], Loss: 0.1213\n",
      "Epoch [12/20], Loss: 0.0958\n",
      "Epoch [13/20], Loss: 0.0681\n",
      "Epoch [14/20], Loss: 0.0565\n",
      "Epoch [15/20], Loss: 0.0497\n",
      "Epoch [16/20], Loss: 0.0501\n",
      "Epoch [17/20], Loss: 0.0546\n",
      "Epoch [18/20], Loss: 0.0450\n",
      "Epoch [19/20], Loss: 0.0431\n",
      "Epoch [20/20], Loss: 0.0650\n",
      "Currently testing: Time steps: 20, Tau: 2.0\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0039\n",
      "Epoch [2/20], Loss: 0.7793\n",
      "Epoch [3/20], Loss: 0.6493\n",
      "Epoch [4/20], Loss: 0.5247\n",
      "Epoch [5/20], Loss: 0.4084\n",
      "Epoch [6/20], Loss: 0.2934\n",
      "Epoch [7/20], Loss: 0.1926\n",
      "Epoch [8/20], Loss: 0.1195\n",
      "Epoch [9/20], Loss: 0.0627\n",
      "Epoch [10/20], Loss: 0.0379\n",
      "Epoch [11/20], Loss: 0.0241\n",
      "Epoch [12/20], Loss: 0.0178\n",
      "Epoch [13/20], Loss: 0.0237\n",
      "Epoch [14/20], Loss: 0.0632\n",
      "Epoch [15/20], Loss: 0.0739\n",
      "Epoch [16/20], Loss: 0.0268\n",
      "Epoch [17/20], Loss: 0.0143\n",
      "Epoch [18/20], Loss: 0.0156\n",
      "Epoch [19/20], Loss: 0.0141\n",
      "Epoch [20/20], Loss: 0.0683\n",
      "Currently testing: Time steps: 20, Tau: 5.0\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0045\n",
      "Epoch [2/20], Loss: 0.7617\n",
      "Epoch [3/20], Loss: 0.6117\n",
      "Epoch [4/20], Loss: 0.4653\n",
      "Epoch [5/20], Loss: 0.3249\n",
      "Epoch [6/20], Loss: 0.2001\n",
      "Epoch [7/20], Loss: 0.1002\n",
      "Epoch [8/20], Loss: 0.0521\n",
      "Epoch [9/20], Loss: 0.0293\n",
      "Epoch [10/20], Loss: 0.0192\n",
      "Epoch [11/20], Loss: 0.0192\n",
      "Epoch [12/20], Loss: 0.0155\n",
      "Epoch [13/20], Loss: 0.0175\n",
      "Epoch [14/20], Loss: 0.0402\n",
      "Epoch [15/20], Loss: 0.0661\n",
      "Epoch [16/20], Loss: 0.0234\n",
      "Epoch [17/20], Loss: 0.0126\n",
      "Epoch [18/20], Loss: 0.0085\n",
      "Epoch [19/20], Loss: 0.0076\n",
      "Epoch [20/20], Loss: 0.0077\n",
      "Currently testing: Time steps: 20, Tau: 10.0\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0111\n",
      "Epoch [2/20], Loss: 0.7398\n",
      "Epoch [3/20], Loss: 0.5816\n",
      "Epoch [4/20], Loss: 0.4198\n",
      "Epoch [5/20], Loss: 0.2616\n",
      "Epoch [6/20], Loss: 0.1332\n",
      "Epoch [7/20], Loss: 0.0617\n",
      "Epoch [8/20], Loss: 0.0309\n",
      "Epoch [9/20], Loss: 0.0204\n",
      "Epoch [10/20], Loss: 0.0186\n",
      "Epoch [11/20], Loss: 0.0195\n",
      "Epoch [12/20], Loss: 0.0159\n",
      "Epoch [13/20], Loss: 0.0146\n",
      "Epoch [14/20], Loss: 0.0139\n",
      "Epoch [15/20], Loss: 0.0158\n",
      "Epoch [16/20], Loss: 0.0571\n",
      "Epoch [17/20], Loss: 0.0570\n",
      "Epoch [18/20], Loss: 0.0184\n",
      "Epoch [19/20], Loss: 0.0101\n",
      "Epoch [20/20], Loss: 0.0063\n",
      "Currently testing: Time steps: 30, Tau: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 12.86 MB\n",
      "Original num elements: 3594880, Final num elements: 107846400\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0032\n",
      "Epoch [2/20], Loss: 0.8244\n",
      "Epoch [3/20], Loss: 0.7190\n",
      "Epoch [4/20], Loss: 0.6209\n",
      "Epoch [5/20], Loss: 0.5328\n",
      "Epoch [6/20], Loss: 0.4391\n",
      "Epoch [7/20], Loss: 0.3592\n",
      "Epoch [8/20], Loss: 0.3009\n",
      "Epoch [9/20], Loss: 0.2358\n",
      "Epoch [10/20], Loss: 0.1725\n",
      "Epoch [11/20], Loss: 0.1286\n",
      "Epoch [12/20], Loss: 0.0899\n",
      "Epoch [13/20], Loss: 0.0681\n",
      "Epoch [14/20], Loss: 0.0670\n",
      "Epoch [15/20], Loss: 0.0658\n",
      "Epoch [16/20], Loss: 0.0584\n",
      "Epoch [17/20], Loss: 0.0691\n",
      "Epoch [18/20], Loss: 0.0420\n",
      "Epoch [19/20], Loss: 0.0429\n",
      "Epoch [20/20], Loss: 0.0655\n",
      "Currently testing: Time steps: 30, Tau: 2.0\n",
      "Original memory: 13.71 MB, Final memory: 12.86 MB\n",
      "Original num elements: 3594880, Final num elements: 107846400\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0014\n",
      "Epoch [2/20], Loss: 0.7840\n",
      "Epoch [3/20], Loss: 0.6483\n",
      "Epoch [4/20], Loss: 0.5143\n",
      "Epoch [5/20], Loss: 0.3805\n",
      "Epoch [6/20], Loss: 0.2545\n",
      "Epoch [7/20], Loss: 0.1628\n",
      "Epoch [8/20], Loss: 0.0898\n",
      "Epoch [9/20], Loss: 0.0502\n",
      "Epoch [10/20], Loss: 0.0402\n",
      "Epoch [11/20], Loss: 0.0276\n",
      "Epoch [12/20], Loss: 0.0157\n",
      "Epoch [13/20], Loss: 0.0213\n",
      "Epoch [14/20], Loss: 0.0937\n",
      "Epoch [15/20], Loss: 0.0999\n",
      "Epoch [16/20], Loss: 0.0303\n",
      "Epoch [17/20], Loss: 0.0130\n",
      "Epoch [18/20], Loss: 0.0086\n",
      "Epoch [19/20], Loss: 0.0056\n",
      "Epoch [20/20], Loss: 0.0498\n",
      "Currently testing: Time steps: 30, Tau: 5.0\n",
      "Original memory: 13.71 MB, Final memory: 12.86 MB\n",
      "Original num elements: 3594880, Final num elements: 107846400\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9845\n",
      "Epoch [2/20], Loss: 0.7318\n",
      "Epoch [3/20], Loss: 0.5613\n",
      "Epoch [4/20], Loss: 0.3936\n",
      "Epoch [5/20], Loss: 0.2436\n",
      "Epoch [6/20], Loss: 0.1214\n",
      "Epoch [7/20], Loss: 0.0513\n",
      "Epoch [8/20], Loss: 0.0353\n",
      "Epoch [9/20], Loss: 0.0221\n",
      "Epoch [10/20], Loss: 0.0175\n",
      "Epoch [11/20], Loss: 0.0161\n",
      "Epoch [12/20], Loss: 0.0187\n",
      "Epoch [13/20], Loss: 0.0244\n",
      "Epoch [14/20], Loss: 0.0971\n",
      "Epoch [15/20], Loss: 0.0440\n",
      "Epoch [16/20], Loss: 0.0206\n",
      "Epoch [17/20], Loss: 0.0081\n",
      "Epoch [18/20], Loss: 0.0091\n",
      "Epoch [19/20], Loss: 0.0071\n",
      "Epoch [20/20], Loss: 0.0062\n",
      "Currently testing: Time steps: 30, Tau: 10.0\n",
      "Original memory: 13.71 MB, Final memory: 12.86 MB\n",
      "Original num elements: 3594880, Final num elements: 107846400\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0064\n",
      "Epoch [2/20], Loss: 0.7290\n",
      "Epoch [3/20], Loss: 0.5602\n",
      "Epoch [4/20], Loss: 0.3928\n",
      "Epoch [5/20], Loss: 0.2258\n",
      "Epoch [6/20], Loss: 0.1087\n",
      "Epoch [7/20], Loss: 0.0513\n",
      "Epoch [8/20], Loss: 0.0246\n",
      "Epoch [9/20], Loss: 0.0187\n",
      "Epoch [10/20], Loss: 0.0139\n",
      "Epoch [11/20], Loss: 0.0149\n",
      "Epoch [12/20], Loss: 0.0122\n",
      "Epoch [13/20], Loss: 0.0097\n",
      "Epoch [14/20], Loss: 0.0160\n",
      "Epoch [15/20], Loss: 0.0923\n",
      "Epoch [16/20], Loss: 0.0641\n",
      "Epoch [17/20], Loss: 0.0168\n",
      "Epoch [18/20], Loss: 0.0089\n",
      "Epoch [19/20], Loss: 0.0063\n",
      "Epoch [20/20], Loss: 0.0058\n",
      "Currently testing: Time steps: 40, Tau: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 17.14 MB\n",
      "Original num elements: 3594880, Final num elements: 143795200\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0052\n",
      "Epoch [2/20], Loss: 0.8289\n",
      "Epoch [3/20], Loss: 0.7224\n",
      "Epoch [4/20], Loss: 0.6282\n",
      "Epoch [5/20], Loss: 0.5509\n",
      "Epoch [6/20], Loss: 0.4566\n",
      "Epoch [7/20], Loss: 0.3662\n",
      "Epoch [8/20], Loss: 0.3025\n",
      "Epoch [9/20], Loss: 0.2315\n",
      "Epoch [10/20], Loss: 0.1855\n",
      "Epoch [11/20], Loss: 0.1279\n",
      "Epoch [12/20], Loss: 0.0982\n",
      "Epoch [13/20], Loss: 0.0815\n",
      "Epoch [14/20], Loss: 0.0799\n",
      "Epoch [15/20], Loss: 0.0736\n",
      "Epoch [16/20], Loss: 0.0582\n",
      "Epoch [17/20], Loss: 0.0613\n",
      "Epoch [18/20], Loss: 0.1093\n",
      "Epoch [19/20], Loss: 0.0340\n",
      "Epoch [20/20], Loss: 0.0587\n",
      "Currently testing: Time steps: 40, Tau: 2.0\n",
      "Original memory: 13.71 MB, Final memory: 17.14 MB\n",
      "Original num elements: 3594880, Final num elements: 143795200\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9929\n",
      "Epoch [2/20], Loss: 0.7707\n",
      "Epoch [3/20], Loss: 0.6299\n",
      "Epoch [4/20], Loss: 0.4996\n",
      "Epoch [5/20], Loss: 0.3687\n",
      "Epoch [6/20], Loss: 0.2402\n",
      "Epoch [7/20], Loss: 0.1576\n",
      "Epoch [8/20], Loss: 0.0908\n",
      "Epoch [9/20], Loss: 0.0560\n",
      "Epoch [10/20], Loss: 0.0395\n",
      "Epoch [11/20], Loss: 0.0311\n",
      "Epoch [12/20], Loss: 0.0458\n",
      "Epoch [13/20], Loss: 0.0877\n",
      "Epoch [14/20], Loss: 0.0545\n",
      "Epoch [15/20], Loss: 0.0799\n",
      "Epoch [16/20], Loss: 0.0640\n",
      "Epoch [17/20], Loss: 0.0143\n",
      "Epoch [18/20], Loss: 0.0101\n",
      "Epoch [19/20], Loss: 0.0101\n",
      "Epoch [20/20], Loss: 0.0166\n",
      "Currently testing: Time steps: 40, Tau: 5.0\n",
      "Original memory: 13.71 MB, Final memory: 17.14 MB\n",
      "Original num elements: 3594880, Final num elements: 143795200\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9878\n",
      "Epoch [2/20], Loss: 0.7235\n",
      "Epoch [3/20], Loss: 0.5466\n",
      "Epoch [4/20], Loss: 0.3708\n",
      "Epoch [5/20], Loss: 0.2076\n",
      "Epoch [6/20], Loss: 0.0973\n",
      "Epoch [7/20], Loss: 0.0425\n",
      "Epoch [8/20], Loss: 0.0231\n",
      "Epoch [9/20], Loss: 0.0163\n",
      "Epoch [10/20], Loss: 0.0136\n",
      "Epoch [11/20], Loss: 0.0159\n",
      "Epoch [12/20], Loss: 0.0514\n",
      "Epoch [13/20], Loss: 0.1249\n",
      "Epoch [14/20], Loss: 0.0388\n",
      "Epoch [15/20], Loss: 0.0128\n",
      "Epoch [16/20], Loss: 0.0096\n",
      "Epoch [17/20], Loss: 0.0061\n",
      "Epoch [18/20], Loss: 0.0085\n",
      "Epoch [19/20], Loss: 0.0078\n",
      "Epoch [20/20], Loss: 0.0141\n",
      "Currently testing: Time steps: 40, Tau: 10.0\n",
      "Original memory: 13.71 MB, Final memory: 17.14 MB\n",
      "Original num elements: 3594880, Final num elements: 143795200\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9917\n",
      "Epoch [2/20], Loss: 0.7039\n",
      "Epoch [3/20], Loss: 0.5117\n",
      "Epoch [4/20], Loss: 0.3080\n",
      "Epoch [5/20], Loss: 0.1448\n",
      "Epoch [6/20], Loss: 0.0585\n",
      "Epoch [7/20], Loss: 0.0267\n",
      "Epoch [8/20], Loss: 0.0175\n",
      "Epoch [9/20], Loss: 0.0126\n",
      "Epoch [10/20], Loss: 0.0131\n",
      "Epoch [11/20], Loss: 0.0104\n",
      "Epoch [12/20], Loss: 0.0119\n",
      "Epoch [13/20], Loss: 0.0148\n",
      "Epoch [14/20], Loss: 0.1343\n",
      "Epoch [15/20], Loss: 0.0663\n",
      "Epoch [16/20], Loss: 0.0217\n",
      "Epoch [17/20], Loss: 0.0084\n",
      "Epoch [18/20], Loss: 0.0080\n",
      "Epoch [19/20], Loss: 0.0065\n",
      "Epoch [20/20], Loss: 0.0077\n",
      "Currently testing: Time steps: 50, Tau: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 21.43 MB\n",
      "Original num elements: 3594880, Final num elements: 179744000\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0266\n",
      "Epoch [2/20], Loss: 0.8404\n",
      "Epoch [3/20], Loss: 0.7409\n",
      "Epoch [4/20], Loss: 0.6508\n",
      "Epoch [5/20], Loss: 0.5562\n",
      "Epoch [6/20], Loss: 0.4832\n",
      "Epoch [7/20], Loss: 0.4060\n",
      "Epoch [8/20], Loss: 0.3158\n",
      "Epoch [9/20], Loss: 0.2578\n",
      "Epoch [10/20], Loss: 0.1979\n",
      "Epoch [11/20], Loss: 0.1545\n",
      "Epoch [12/20], Loss: 0.1210\n",
      "Epoch [13/20], Loss: 0.1066\n",
      "Epoch [14/20], Loss: 0.0794\n",
      "Epoch [15/20], Loss: 0.0933\n",
      "Epoch [16/20], Loss: 0.1037\n",
      "Epoch [17/20], Loss: 0.0637\n",
      "Epoch [18/20], Loss: 0.0696\n",
      "Epoch [19/20], Loss: 0.0496\n",
      "Epoch [20/20], Loss: 0.0548\n",
      "Currently testing: Time steps: 50, Tau: 2.0\n",
      "Original memory: 13.71 MB, Final memory: 21.43 MB\n",
      "Original num elements: 3594880, Final num elements: 179744000\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9940\n",
      "Epoch [2/20], Loss: 0.7803\n",
      "Epoch [3/20], Loss: 0.6481\n",
      "Epoch [4/20], Loss: 0.5181\n",
      "Epoch [5/20], Loss: 0.3931\n",
      "Epoch [6/20], Loss: 0.2796\n",
      "Epoch [7/20], Loss: 0.1838\n",
      "Epoch [8/20], Loss: 0.1116\n",
      "Epoch [9/20], Loss: 0.0663\n",
      "Epoch [10/20], Loss: 0.0500\n",
      "Epoch [11/20], Loss: 0.0422\n",
      "Epoch [12/20], Loss: 0.0506\n",
      "Epoch [13/20], Loss: 0.0713\n",
      "Epoch [14/20], Loss: 0.0654\n",
      "Epoch [15/20], Loss: 0.0369\n",
      "Epoch [16/20], Loss: 0.0357\n",
      "Epoch [17/20], Loss: 0.0472\n",
      "Epoch [18/20], Loss: 0.0339\n",
      "Epoch [19/20], Loss: 0.0360\n",
      "Epoch [20/20], Loss: 0.0344\n",
      "Currently testing: Time steps: 50, Tau: 5.0\n",
      "Original memory: 13.71 MB, Final memory: 21.43 MB\n",
      "Original num elements: 3594880, Final num elements: 179744000\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9868\n",
      "Epoch [2/20], Loss: 0.7184\n",
      "Epoch [3/20], Loss: 0.5379\n",
      "Epoch [4/20], Loss: 0.3523\n",
      "Epoch [5/20], Loss: 0.1931\n",
      "Epoch [6/20], Loss: 0.0899\n",
      "Epoch [7/20], Loss: 0.0439\n",
      "Epoch [8/20], Loss: 0.0218\n",
      "Epoch [9/20], Loss: 0.0165\n",
      "Epoch [10/20], Loss: 0.0181\n",
      "Epoch [11/20], Loss: 0.0130\n",
      "Epoch [12/20], Loss: 0.0193\n",
      "Epoch [13/20], Loss: 0.1669\n",
      "Epoch [14/20], Loss: 0.0713\n",
      "Epoch [15/20], Loss: 0.0225\n",
      "Epoch [16/20], Loss: 0.0128\n",
      "Epoch [17/20], Loss: 0.0061\n",
      "Epoch [18/20], Loss: 0.0054\n",
      "Epoch [19/20], Loss: 0.0051\n",
      "Epoch [20/20], Loss: 0.0047\n",
      "Currently testing: Time steps: 50, Tau: 10.0\n",
      "Original memory: 13.71 MB, Final memory: 21.43 MB\n",
      "Original num elements: 3594880, Final num elements: 179744000\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9907\n",
      "Epoch [2/20], Loss: 0.6938\n",
      "Epoch [3/20], Loss: 0.4795\n",
      "Epoch [4/20], Loss: 0.2679\n",
      "Epoch [5/20], Loss: 0.1174\n",
      "Epoch [6/20], Loss: 0.0495\n",
      "Epoch [7/20], Loss: 0.0223\n",
      "Epoch [8/20], Loss: 0.0139\n",
      "Epoch [9/20], Loss: 0.0135\n",
      "Epoch [10/20], Loss: 0.0187\n",
      "Epoch [11/20], Loss: 0.0174\n",
      "Epoch [12/20], Loss: 0.0276\n",
      "Epoch [13/20], Loss: 0.1257\n",
      "Epoch [14/20], Loss: 0.0618\n",
      "Epoch [15/20], Loss: 0.0199\n",
      "Epoch [16/20], Loss: 0.0089\n",
      "Epoch [17/20], Loss: 0.0068\n",
      "Epoch [18/20], Loss: 0.0061\n",
      "Epoch [19/20], Loss: 0.0061\n",
      "Epoch [20/20], Loss: 0.0050\n",
      "Currently testing: Time steps: 60, Tau: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 25.71 MB\n",
      "Original num elements: 3594880, Final num elements: 215692800\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0302\n",
      "Epoch [2/20], Loss: 0.8425\n",
      "Epoch [3/20], Loss: 0.7510\n",
      "Epoch [4/20], Loss: 0.6607\n",
      "Epoch [5/20], Loss: 0.5706\n",
      "Epoch [6/20], Loss: 0.4894\n",
      "Epoch [7/20], Loss: 0.4166\n",
      "Epoch [8/20], Loss: 0.3541\n",
      "Epoch [9/20], Loss: 0.2823\n",
      "Epoch [10/20], Loss: 0.2272\n",
      "Epoch [11/20], Loss: 0.1878\n",
      "Epoch [12/20], Loss: 0.1416\n",
      "Epoch [13/20], Loss: 0.1622\n",
      "Epoch [14/20], Loss: 0.1332\n",
      "Epoch [15/20], Loss: 0.0764\n",
      "Epoch [16/20], Loss: 0.0829\n",
      "Epoch [17/20], Loss: 0.0567\n",
      "Epoch [18/20], Loss: 0.0767\n",
      "Epoch [19/20], Loss: 0.0639\n",
      "Epoch [20/20], Loss: 0.0774\n",
      "Currently testing: Time steps: 60, Tau: 2.0\n",
      "Original memory: 13.71 MB, Final memory: 25.71 MB\n",
      "Original num elements: 3594880, Final num elements: 215692800\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0138\n",
      "Epoch [2/20], Loss: 0.7929\n",
      "Epoch [3/20], Loss: 0.6538\n",
      "Epoch [4/20], Loss: 0.5253\n",
      "Epoch [5/20], Loss: 0.4204\n",
      "Epoch [6/20], Loss: 0.2826\n",
      "Epoch [7/20], Loss: 0.1884\n",
      "Epoch [8/20], Loss: 0.1185\n",
      "Epoch [9/20], Loss: 0.0716\n",
      "Epoch [10/20], Loss: 0.0486\n",
      "Epoch [11/20], Loss: 0.0420\n",
      "Epoch [12/20], Loss: 0.0606\n",
      "Epoch [13/20], Loss: 0.0724\n",
      "Epoch [14/20], Loss: 0.0665\n",
      "Epoch [15/20], Loss: 0.0494\n",
      "Epoch [16/20], Loss: 0.0432\n",
      "Epoch [17/20], Loss: 0.0236\n",
      "Epoch [18/20], Loss: 0.0430\n",
      "Epoch [19/20], Loss: 0.0728\n",
      "Epoch [20/20], Loss: 0.0390\n",
      "Currently testing: Time steps: 60, Tau: 5.0\n",
      "Original memory: 13.71 MB, Final memory: 25.71 MB\n",
      "Original num elements: 3594880, Final num elements: 215692800\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9948\n",
      "Epoch [2/20], Loss: 0.7375\n",
      "Epoch [3/20], Loss: 0.5746\n",
      "Epoch [4/20], Loss: 0.3951\n",
      "Epoch [5/20], Loss: 0.2414\n",
      "Epoch [6/20], Loss: 0.1278\n",
      "Epoch [7/20], Loss: 0.0777\n",
      "Epoch [8/20], Loss: 0.0313\n",
      "Epoch [9/20], Loss: 0.0186\n",
      "Epoch [10/20], Loss: 0.0200\n",
      "Epoch [11/20], Loss: 0.0219\n",
      "Epoch [12/20], Loss: 0.0647\n",
      "Epoch [13/20], Loss: 0.1268\n",
      "Epoch [14/20], Loss: 0.0622\n",
      "Epoch [15/20], Loss: 0.0204\n",
      "Epoch [16/20], Loss: 0.0086\n",
      "Epoch [17/20], Loss: 0.0070\n",
      "Epoch [18/20], Loss: 0.0076\n",
      "Epoch [19/20], Loss: 0.0052\n",
      "Epoch [20/20], Loss: 0.0048\n",
      "Currently testing: Time steps: 60, Tau: 10.0\n",
      "Original memory: 13.71 MB, Final memory: 25.71 MB\n",
      "Original num elements: 3594880, Final num elements: 215692800\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9864\n",
      "Epoch [2/20], Loss: 0.6919\n",
      "Epoch [3/20], Loss: 0.4760\n",
      "Epoch [4/20], Loss: 0.2634\n",
      "Epoch [5/20], Loss: 0.1133\n",
      "Epoch [6/20], Loss: 0.0436\n",
      "Epoch [7/20], Loss: 0.0217\n",
      "Epoch [8/20], Loss: 0.0140\n",
      "Epoch [9/20], Loss: 0.0144\n",
      "Epoch [10/20], Loss: 0.0098\n",
      "Epoch [11/20], Loss: 0.0104\n",
      "Epoch [12/20], Loss: 0.0164\n",
      "Epoch [13/20], Loss: 0.1903\n",
      "Epoch [14/20], Loss: 0.0889\n",
      "Epoch [15/20], Loss: 0.0163\n",
      "Epoch [16/20], Loss: 0.0091\n",
      "Epoch [17/20], Loss: 0.0069\n",
      "Epoch [18/20], Loss: 0.0061\n",
      "Epoch [19/20], Loss: 0.0051\n",
      "Epoch [20/20], Loss: 0.0058\n",
      "Time steps: 10, Tau: 1.0, Accuracy: 70.68%, Time: 6.37 seconds\n",
      "Time steps: 10, Tau: 2.0, Accuracy: 72.78%, Time: 5.64 seconds\n",
      "Time steps: 10, Tau: 5.0, Accuracy: 72.19%, Time: 6.07 seconds\n",
      "Time steps: 10, Tau: 10.0, Accuracy: 71.69%, Time: 6.18 seconds\n",
      "Time steps: 20, Tau: 1.0, Accuracy: 71.69%, Time: 9.97 seconds\n",
      "Time steps: 20, Tau: 2.0, Accuracy: 70.70%, Time: 10.27 seconds\n",
      "Time steps: 20, Tau: 5.0, Accuracy: 71.66%, Time: 9.89 seconds\n",
      "Time steps: 20, Tau: 10.0, Accuracy: 70.79%, Time: 10.90 seconds\n",
      "Time steps: 30, Tau: 1.0, Accuracy: 71.44%, Time: 13.65 seconds\n",
      "Time steps: 30, Tau: 2.0, Accuracy: 71.25%, Time: 14.55 seconds\n",
      "Time steps: 30, Tau: 5.0, Accuracy: 72.32%, Time: 13.90 seconds\n",
      "Time steps: 30, Tau: 10.0, Accuracy: 71.60%, Time: 14.68 seconds\n",
      "Time steps: 40, Tau: 1.0, Accuracy: 71.98%, Time: 18.17 seconds\n",
      "Time steps: 40, Tau: 2.0, Accuracy: 70.23%, Time: 17.50 seconds\n",
      "Time steps: 40, Tau: 5.0, Accuracy: 68.84%, Time: 19.53 seconds\n",
      "Time steps: 40, Tau: 10.0, Accuracy: 70.70%, Time: 18.08 seconds\n",
      "Time steps: 50, Tau: 1.0, Accuracy: 71.21%, Time: 24.50 seconds\n",
      "Time steps: 50, Tau: 2.0, Accuracy: 71.44%, Time: 24.90 seconds\n",
      "Time steps: 50, Tau: 5.0, Accuracy: 72.44%, Time: 25.66 seconds\n",
      "Time steps: 50, Tau: 10.0, Accuracy: 71.27%, Time: 24.48 seconds\n",
      "Time steps: 60, Tau: 1.0, Accuracy: 69.86%, Time: 28.16 seconds\n",
      "Time steps: 60, Tau: 2.0, Accuracy: 70.11%, Time: 28.40 seconds\n",
      "Time steps: 60, Tau: 5.0, Accuracy: 71.69%, Time: 28.50 seconds\n",
      "Time steps: 60, Tau: 10.0, Accuracy: 71.75%, Time: 26.31 seconds\n"
     ]
    }
   ],
   "source": [
    "# Number of time steps and tau\n",
    "time_steps_list = [10, 20, 30, 40, 50, 60]\n",
    "tau_list = [1.0, 2.0, 5.0, 10.0]\n",
    "acc_list = []\n",
    "time_list = []\n",
    "\n",
    "for time_steps in time_steps_list:\n",
    "  for tau in tau_list:\n",
    "    print(f\"Currently testing: Time steps: {time_steps}, Tau: {tau}\")\n",
    "    baseline_hyperparameters[\"time_steps\"] = time_steps\n",
    "    baseline_hyperparameters[\"tau\"] = tau\n",
    "    acc, time_taken = get_results(baseline_hyperparameters, system_params)\n",
    "    acc_list.append(acc)\n",
    "    time_list.append(time_taken)\n",
    "\n",
    "idx = 0\n",
    "for time_steps in time_steps_list:\n",
    "  for tau in tau_list:\n",
    "    print(f\"Time steps: {time_steps}, Tau: {tau}, Accuracy: {acc_list[idx]:.2f}%, Time: {time_list[idx]:.2f} seconds\")\n",
    "    idx += 1\n",
    "\n",
    "baseline_hyperparameters = baseline_hyperparameters_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently testing: a: 0.1, b: 0.9, Prune: None\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0271\n",
      "Epoch [2/20], Loss: 0.8228\n",
      "Epoch [3/20], Loss: 0.6999\n",
      "Epoch [4/20], Loss: 0.5950\n",
      "Epoch [5/20], Loss: 0.4880\n",
      "Epoch [6/20], Loss: 0.3970\n",
      "Epoch [7/20], Loss: 0.3195\n",
      "Epoch [8/20], Loss: 0.2603\n",
      "Epoch [9/20], Loss: 0.1867\n",
      "Epoch [10/20], Loss: 0.1387\n",
      "Epoch [11/20], Loss: 0.1038\n",
      "Epoch [12/20], Loss: 0.0830\n",
      "Epoch [13/20], Loss: 0.0647\n",
      "Epoch [14/20], Loss: 0.0564\n",
      "Epoch [15/20], Loss: 0.0495\n",
      "Epoch [16/20], Loss: 0.0543\n",
      "Epoch [17/20], Loss: 0.0733\n",
      "Epoch [18/20], Loss: 0.0350\n",
      "Epoch [19/20], Loss: 0.0380\n",
      "Epoch [20/20], Loss: 0.0475\n",
      "Currently testing: a: 0.1, b: 0.9, Prune: 0.6\n",
      "Original memory: 13.71 MB, Final memory: 7.71 MB\n",
      "Original num elements: 3594880, Final num elements: 64707840\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0347\n",
      "Epoch [2/20], Loss: 0.8422\n",
      "Epoch [3/20], Loss: 0.7270\n",
      "Epoch [4/20], Loss: 0.6220\n",
      "Epoch [5/20], Loss: 0.5181\n",
      "Epoch [6/20], Loss: 0.4355\n",
      "Epoch [7/20], Loss: 0.3518\n",
      "Epoch [8/20], Loss: 0.2670\n",
      "Epoch [9/20], Loss: 0.2136\n",
      "Epoch [10/20], Loss: 0.1567\n",
      "Epoch [11/20], Loss: 0.1308\n",
      "Epoch [12/20], Loss: 0.0961\n",
      "Epoch [13/20], Loss: 0.0800\n",
      "Epoch [14/20], Loss: 0.0657\n",
      "Epoch [15/20], Loss: 0.0511\n",
      "Epoch [16/20], Loss: 0.0622\n",
      "Epoch [17/20], Loss: 0.0464\n",
      "Epoch [18/20], Loss: 0.0384\n",
      "Epoch [19/20], Loss: 0.0564\n",
      "Epoch [20/20], Loss: 0.0500\n",
      "Currently testing: a: 0.1, b: 0.9, Prune: 0.8\n",
      "Original memory: 13.71 MB, Final memory: 7.29 MB\n",
      "Original num elements: 3594880, Final num elements: 61112960\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0436\n",
      "Epoch [2/20], Loss: 0.8590\n",
      "Epoch [3/20], Loss: 0.7501\n",
      "Epoch [4/20], Loss: 0.6462\n",
      "Epoch [5/20], Loss: 0.5568\n",
      "Epoch [6/20], Loss: 0.4705\n",
      "Epoch [7/20], Loss: 0.3795\n",
      "Epoch [8/20], Loss: 0.3072\n",
      "Epoch [9/20], Loss: 0.2345\n",
      "Epoch [10/20], Loss: 0.1845\n",
      "Epoch [11/20], Loss: 0.1365\n",
      "Epoch [12/20], Loss: 0.1101\n",
      "Epoch [13/20], Loss: 0.0888\n",
      "Epoch [14/20], Loss: 0.0719\n",
      "Epoch [15/20], Loss: 0.0560\n",
      "Epoch [16/20], Loss: 0.0503\n",
      "Epoch [17/20], Loss: 0.0640\n",
      "Epoch [18/20], Loss: 0.0379\n",
      "Epoch [19/20], Loss: 0.0225\n",
      "Epoch [20/20], Loss: 0.0603\n",
      "Currently testing: a: 0.1, b: 0.9, Prune: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 6.00 MB\n",
      "Original num elements: 3594880, Final num elements: 50328320\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0471\n",
      "Epoch [2/20], Loss: 0.8659\n",
      "Epoch [3/20], Loss: 0.7609\n",
      "Epoch [4/20], Loss: 0.6643\n",
      "Epoch [5/20], Loss: 0.5697\n",
      "Epoch [6/20], Loss: 0.4816\n",
      "Epoch [7/20], Loss: 0.3939\n",
      "Epoch [8/20], Loss: 0.3154\n",
      "Epoch [9/20], Loss: 0.2540\n",
      "Epoch [10/20], Loss: 0.1936\n",
      "Epoch [11/20], Loss: 0.1594\n",
      "Epoch [12/20], Loss: 0.1125\n",
      "Epoch [13/20], Loss: 0.0862\n",
      "Epoch [14/20], Loss: 0.0794\n",
      "Epoch [15/20], Loss: 0.0660\n",
      "Epoch [16/20], Loss: 0.0518\n",
      "Epoch [17/20], Loss: 0.0408\n",
      "Epoch [18/20], Loss: 0.0423\n",
      "Epoch [19/20], Loss: 0.0464\n",
      "Epoch [20/20], Loss: 0.0511\n",
      "Currently testing: a: 0.3, b: 0.7, Prune: None\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0093\n",
      "Epoch [2/20], Loss: 0.8209\n",
      "Epoch [3/20], Loss: 0.7094\n",
      "Epoch [4/20], Loss: 0.6085\n",
      "Epoch [5/20], Loss: 0.5170\n",
      "Epoch [6/20], Loss: 0.4216\n",
      "Epoch [7/20], Loss: 0.3446\n",
      "Epoch [8/20], Loss: 0.2743\n",
      "Epoch [9/20], Loss: 0.1965\n",
      "Epoch [10/20], Loss: 0.1682\n",
      "Epoch [11/20], Loss: 0.1108\n",
      "Epoch [12/20], Loss: 0.0806\n",
      "Epoch [13/20], Loss: 0.0602\n",
      "Epoch [14/20], Loss: 0.0698\n",
      "Epoch [15/20], Loss: 0.0451\n",
      "Epoch [16/20], Loss: 0.0412\n",
      "Epoch [17/20], Loss: 0.0520\n",
      "Epoch [18/20], Loss: 0.0531\n",
      "Epoch [19/20], Loss: 0.0618\n",
      "Epoch [20/20], Loss: 0.0715\n",
      "Currently testing: a: 0.3, b: 0.7, Prune: 0.6\n",
      "Original memory: 13.71 MB, Final memory: 7.71 MB\n",
      "Original num elements: 3594880, Final num elements: 64707840\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0275\n",
      "Epoch [2/20], Loss: 0.8497\n",
      "Epoch [3/20], Loss: 0.7441\n",
      "Epoch [4/20], Loss: 0.6583\n",
      "Epoch [5/20], Loss: 0.5655\n",
      "Epoch [6/20], Loss: 0.4684\n",
      "Epoch [7/20], Loss: 0.3985\n",
      "Epoch [8/20], Loss: 0.3281\n",
      "Epoch [9/20], Loss: 0.2439\n",
      "Epoch [10/20], Loss: 0.1955\n",
      "Epoch [11/20], Loss: 0.1481\n",
      "Epoch [12/20], Loss: 0.1159\n",
      "Epoch [13/20], Loss: 0.0920\n",
      "Epoch [14/20], Loss: 0.0782\n",
      "Epoch [15/20], Loss: 0.0549\n",
      "Epoch [16/20], Loss: 0.0505\n",
      "Epoch [17/20], Loss: 0.0763\n",
      "Epoch [18/20], Loss: 0.0365\n",
      "Epoch [19/20], Loss: 0.0355\n",
      "Epoch [20/20], Loss: 0.0424\n",
      "Currently testing: a: 0.3, b: 0.7, Prune: 0.8\n",
      "Original memory: 13.71 MB, Final memory: 7.29 MB\n",
      "Original num elements: 3594880, Final num elements: 61112960\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0235\n",
      "Epoch [2/20], Loss: 0.8487\n",
      "Epoch [3/20], Loss: 0.7469\n",
      "Epoch [4/20], Loss: 0.6572\n",
      "Epoch [5/20], Loss: 0.5690\n",
      "Epoch [6/20], Loss: 0.4777\n",
      "Epoch [7/20], Loss: 0.3900\n",
      "Epoch [8/20], Loss: 0.3103\n",
      "Epoch [9/20], Loss: 0.2356\n",
      "Epoch [10/20], Loss: 0.1836\n",
      "Epoch [11/20], Loss: 0.1370\n",
      "Epoch [12/20], Loss: 0.1383\n",
      "Epoch [13/20], Loss: 0.0795\n",
      "Epoch [14/20], Loss: 0.0579\n",
      "Epoch [15/20], Loss: 0.0536\n",
      "Epoch [16/20], Loss: 0.0397\n",
      "Epoch [17/20], Loss: 0.0583\n",
      "Epoch [18/20], Loss: 0.0382\n",
      "Epoch [19/20], Loss: 0.0695\n",
      "Epoch [20/20], Loss: 0.0448\n",
      "Currently testing: a: 0.3, b: 0.7, Prune: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 6.00 MB\n",
      "Original num elements: 3594880, Final num elements: 50328320\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0283\n",
      "Epoch [2/20], Loss: 0.8581\n",
      "Epoch [3/20], Loss: 0.7619\n",
      "Epoch [4/20], Loss: 0.6740\n",
      "Epoch [5/20], Loss: 0.5838\n",
      "Epoch [6/20], Loss: 0.5003\n",
      "Epoch [7/20], Loss: 0.4217\n",
      "Epoch [8/20], Loss: 0.3466\n",
      "Epoch [9/20], Loss: 0.2719\n",
      "Epoch [10/20], Loss: 0.2388\n",
      "Epoch [11/20], Loss: 0.1680\n",
      "Epoch [12/20], Loss: 0.1415\n",
      "Epoch [13/20], Loss: 0.0961\n",
      "Epoch [14/20], Loss: 0.0706\n",
      "Epoch [15/20], Loss: 0.0592\n",
      "Epoch [16/20], Loss: 0.0439\n",
      "Epoch [17/20], Loss: 0.0509\n",
      "Epoch [18/20], Loss: 0.0651\n",
      "Epoch [19/20], Loss: 0.0555\n",
      "Epoch [20/20], Loss: 0.0458\n",
      "Currently testing: a: 0.5, b: 0.5, Prune: None\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9984\n",
      "Epoch [2/20], Loss: 0.8222\n",
      "Epoch [3/20], Loss: 0.7058\n",
      "Epoch [4/20], Loss: 0.6106\n",
      "Epoch [5/20], Loss: 0.5187\n",
      "Epoch [6/20], Loss: 0.4315\n",
      "Epoch [7/20], Loss: 0.3460\n",
      "Epoch [8/20], Loss: 0.2712\n",
      "Epoch [9/20], Loss: 0.2024\n",
      "Epoch [10/20], Loss: 0.1516\n",
      "Epoch [11/20], Loss: 0.1139\n",
      "Epoch [12/20], Loss: 0.0835\n",
      "Epoch [13/20], Loss: 0.0580\n",
      "Epoch [14/20], Loss: 0.0642\n",
      "Epoch [15/20], Loss: 0.0607\n",
      "Epoch [16/20], Loss: 0.0479\n",
      "Epoch [17/20], Loss: 0.0350\n",
      "Epoch [18/20], Loss: 0.0419\n",
      "Epoch [19/20], Loss: 0.0454\n",
      "Epoch [20/20], Loss: 0.0447\n",
      "Currently testing: a: 0.5, b: 0.5, Prune: 0.6\n",
      "Original memory: 13.71 MB, Final memory: 7.71 MB\n",
      "Original num elements: 3594880, Final num elements: 64707840\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0124\n",
      "Epoch [2/20], Loss: 0.8416\n",
      "Epoch [3/20], Loss: 0.7461\n",
      "Epoch [4/20], Loss: 0.6541\n",
      "Epoch [5/20], Loss: 0.5622\n",
      "Epoch [6/20], Loss: 0.4785\n",
      "Epoch [7/20], Loss: 0.3934\n",
      "Epoch [8/20], Loss: 0.3116\n",
      "Epoch [9/20], Loss: 0.2560\n",
      "Epoch [10/20], Loss: 0.1942\n",
      "Epoch [11/20], Loss: 0.1410\n",
      "Epoch [12/20], Loss: 0.1118\n",
      "Epoch [13/20], Loss: 0.0872\n",
      "Epoch [14/20], Loss: 0.0786\n",
      "Epoch [15/20], Loss: 0.0426\n",
      "Epoch [16/20], Loss: 0.0420\n",
      "Epoch [17/20], Loss: 0.0523\n",
      "Epoch [18/20], Loss: 0.0715\n",
      "Epoch [19/20], Loss: 0.0526\n",
      "Epoch [20/20], Loss: 0.0334\n",
      "Currently testing: a: 0.5, b: 0.5, Prune: 0.8\n",
      "Original memory: 13.71 MB, Final memory: 7.29 MB\n",
      "Original num elements: 3594880, Final num elements: 61112960\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0077\n",
      "Epoch [2/20], Loss: 0.8374\n",
      "Epoch [3/20], Loss: 0.7408\n",
      "Epoch [4/20], Loss: 0.6580\n",
      "Epoch [5/20], Loss: 0.5673\n",
      "Epoch [6/20], Loss: 0.4837\n",
      "Epoch [7/20], Loss: 0.4125\n",
      "Epoch [8/20], Loss: 0.3445\n",
      "Epoch [9/20], Loss: 0.2600\n",
      "Epoch [10/20], Loss: 0.2104\n",
      "Epoch [11/20], Loss: 0.1672\n",
      "Epoch [12/20], Loss: 0.1179\n",
      "Epoch [13/20], Loss: 0.0963\n",
      "Epoch [14/20], Loss: 0.0724\n",
      "Epoch [15/20], Loss: 0.0592\n",
      "Epoch [16/20], Loss: 0.0528\n",
      "Epoch [17/20], Loss: 0.0434\n",
      "Epoch [18/20], Loss: 0.0437\n",
      "Epoch [19/20], Loss: 0.0495\n",
      "Epoch [20/20], Loss: 0.0568\n",
      "Currently testing: a: 0.5, b: 0.5, Prune: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 6.00 MB\n",
      "Original num elements: 3594880, Final num elements: 50328320\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0200\n",
      "Epoch [2/20], Loss: 0.8613\n",
      "Epoch [3/20], Loss: 0.7641\n",
      "Epoch [4/20], Loss: 0.6811\n",
      "Epoch [5/20], Loss: 0.6029\n",
      "Epoch [6/20], Loss: 0.5222\n",
      "Epoch [7/20], Loss: 0.4466\n",
      "Epoch [8/20], Loss: 0.3854\n",
      "Epoch [9/20], Loss: 0.3159\n",
      "Epoch [10/20], Loss: 0.2456\n",
      "Epoch [11/20], Loss: 0.1936\n",
      "Epoch [12/20], Loss: 0.1545\n",
      "Epoch [13/20], Loss: 0.1090\n",
      "Epoch [14/20], Loss: 0.0890\n",
      "Epoch [15/20], Loss: 0.0682\n",
      "Epoch [16/20], Loss: 0.0609\n",
      "Epoch [17/20], Loss: 0.0382\n",
      "Epoch [18/20], Loss: 0.0352\n",
      "Epoch [19/20], Loss: 0.0649\n",
      "Epoch [20/20], Loss: 0.0470\n",
      "Currently testing: a: 0.7, b: 0.30000000000000004, Prune: None\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9883\n",
      "Epoch [2/20], Loss: 0.8243\n",
      "Epoch [3/20], Loss: 0.7233\n",
      "Epoch [4/20], Loss: 0.6303\n",
      "Epoch [5/20], Loss: 0.5491\n",
      "Epoch [6/20], Loss: 0.4658\n",
      "Epoch [7/20], Loss: 0.3946\n",
      "Epoch [8/20], Loss: 0.3225\n",
      "Epoch [9/20], Loss: 0.2572\n",
      "Epoch [10/20], Loss: 0.1964\n",
      "Epoch [11/20], Loss: 0.1456\n",
      "Epoch [12/20], Loss: 0.1086\n",
      "Epoch [13/20], Loss: 0.0792\n",
      "Epoch [14/20], Loss: 0.0607\n",
      "Epoch [15/20], Loss: 0.0530\n",
      "Epoch [16/20], Loss: 0.0682\n",
      "Epoch [17/20], Loss: 0.0674\n",
      "Epoch [18/20], Loss: 0.0404\n",
      "Epoch [19/20], Loss: 0.0555\n",
      "Epoch [20/20], Loss: 0.0407\n",
      "Currently testing: a: 0.7, b: 0.30000000000000004, Prune: 0.6\n",
      "Original memory: 13.71 MB, Final memory: 7.71 MB\n",
      "Original num elements: 3594880, Final num elements: 64707840\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0005\n",
      "Epoch [2/20], Loss: 0.8382\n",
      "Epoch [3/20], Loss: 0.7364\n",
      "Epoch [4/20], Loss: 0.6503\n",
      "Epoch [5/20], Loss: 0.5676\n",
      "Epoch [6/20], Loss: 0.4894\n",
      "Epoch [7/20], Loss: 0.4170\n",
      "Epoch [8/20], Loss: 0.3443\n",
      "Epoch [9/20], Loss: 0.2906\n",
      "Epoch [10/20], Loss: 0.2251\n",
      "Epoch [11/20], Loss: 0.1667\n",
      "Epoch [12/20], Loss: 0.1215\n",
      "Epoch [13/20], Loss: 0.0963\n",
      "Epoch [14/20], Loss: 0.0765\n",
      "Epoch [15/20], Loss: 0.0678\n",
      "Epoch [16/20], Loss: 0.0500\n",
      "Epoch [17/20], Loss: 0.0518\n",
      "Epoch [18/20], Loss: 0.0463\n",
      "Epoch [19/20], Loss: 0.0726\n",
      "Epoch [20/20], Loss: 0.0472\n",
      "Currently testing: a: 0.7, b: 0.30000000000000004, Prune: 0.8\n",
      "Original memory: 13.71 MB, Final memory: 7.29 MB\n",
      "Original num elements: 3594880, Final num elements: 61112960\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0071\n",
      "Epoch [2/20], Loss: 0.8506\n",
      "Epoch [3/20], Loss: 0.7561\n",
      "Epoch [4/20], Loss: 0.6754\n",
      "Epoch [5/20], Loss: 0.5899\n",
      "Epoch [6/20], Loss: 0.5171\n",
      "Epoch [7/20], Loss: 0.4502\n",
      "Epoch [8/20], Loss: 0.3736\n",
      "Epoch [9/20], Loss: 0.2978\n",
      "Epoch [10/20], Loss: 0.2446\n",
      "Epoch [11/20], Loss: 0.1830\n",
      "Epoch [12/20], Loss: 0.1450\n",
      "Epoch [13/20], Loss: 0.1144\n",
      "Epoch [14/20], Loss: 0.0878\n",
      "Epoch [15/20], Loss: 0.0676\n",
      "Epoch [16/20], Loss: 0.0593\n",
      "Epoch [17/20], Loss: 0.0674\n",
      "Epoch [18/20], Loss: 0.0457\n",
      "Epoch [19/20], Loss: 0.0466\n",
      "Epoch [20/20], Loss: 0.0425\n",
      "Currently testing: a: 0.7, b: 0.30000000000000004, Prune: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 6.00 MB\n",
      "Original num elements: 3594880, Final num elements: 50328320\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0032\n",
      "Epoch [2/20], Loss: 0.8513\n",
      "Epoch [3/20], Loss: 0.7585\n",
      "Epoch [4/20], Loss: 0.6724\n",
      "Epoch [5/20], Loss: 0.5968\n",
      "Epoch [6/20], Loss: 0.5188\n",
      "Epoch [7/20], Loss: 0.4354\n",
      "Epoch [8/20], Loss: 0.3683\n",
      "Epoch [9/20], Loss: 0.2993\n",
      "Epoch [10/20], Loss: 0.2373\n",
      "Epoch [11/20], Loss: 0.1831\n",
      "Epoch [12/20], Loss: 0.1434\n",
      "Epoch [13/20], Loss: 0.1151\n",
      "Epoch [14/20], Loss: 0.0906\n",
      "Epoch [15/20], Loss: 0.0613\n",
      "Epoch [16/20], Loss: 0.0457\n",
      "Epoch [17/20], Loss: 0.0524\n",
      "Epoch [18/20], Loss: 0.0498\n",
      "Epoch [19/20], Loss: 0.0438\n",
      "Epoch [20/20], Loss: 0.0339\n",
      "Currently testing: a: 0.9, b: 0.09999999999999998, Prune: None\n",
      "Original memory: 13.71 MB, Final memory: 8.57 MB\n",
      "Original num elements: 3594880, Final num elements: 71897600\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 0.9877\n",
      "Epoch [2/20], Loss: 0.8225\n",
      "Epoch [3/20], Loss: 0.7313\n",
      "Epoch [4/20], Loss: 0.6377\n",
      "Epoch [5/20], Loss: 0.5653\n",
      "Epoch [6/20], Loss: 0.4909\n",
      "Epoch [7/20], Loss: 0.4208\n",
      "Epoch [8/20], Loss: 0.3433\n",
      "Epoch [9/20], Loss: 0.2804\n",
      "Epoch [10/20], Loss: 0.2335\n",
      "Epoch [11/20], Loss: 0.1808\n",
      "Epoch [12/20], Loss: 0.1301\n",
      "Epoch [13/20], Loss: 0.1016\n",
      "Epoch [14/20], Loss: 0.0886\n",
      "Epoch [15/20], Loss: 0.0576\n",
      "Epoch [16/20], Loss: 0.0731\n",
      "Epoch [17/20], Loss: 0.0531\n",
      "Epoch [18/20], Loss: 0.0506\n",
      "Epoch [19/20], Loss: 0.0491\n",
      "Epoch [20/20], Loss: 0.0595\n",
      "Currently testing: a: 0.9, b: 0.09999999999999998, Prune: 0.6\n",
      "Original memory: 13.71 MB, Final memory: 8.14 MB\n",
      "Original num elements: 3594880, Final num elements: 68302720\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0033\n",
      "Epoch [2/20], Loss: 0.8360\n",
      "Epoch [3/20], Loss: 0.7409\n",
      "Epoch [4/20], Loss: 0.6613\n",
      "Epoch [5/20], Loss: 0.5775\n",
      "Epoch [6/20], Loss: 0.5108\n",
      "Epoch [7/20], Loss: 0.4310\n",
      "Epoch [8/20], Loss: 0.3605\n",
      "Epoch [9/20], Loss: 0.3134\n",
      "Epoch [10/20], Loss: 0.2527\n",
      "Epoch [11/20], Loss: 0.1871\n",
      "Epoch [12/20], Loss: 0.1410\n",
      "Epoch [13/20], Loss: 0.1029\n",
      "Epoch [14/20], Loss: 0.0839\n",
      "Epoch [15/20], Loss: 0.0693\n",
      "Epoch [16/20], Loss: 0.0647\n",
      "Epoch [17/20], Loss: 0.0532\n",
      "Epoch [18/20], Loss: 0.0425\n",
      "Epoch [19/20], Loss: 0.0750\n",
      "Epoch [20/20], Loss: 0.0765\n",
      "Currently testing: a: 0.9, b: 0.09999999999999998, Prune: 0.8\n",
      "Original memory: 13.71 MB, Final memory: 7.71 MB\n",
      "Original num elements: 3594880, Final num elements: 64707840\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0041\n",
      "Epoch [2/20], Loss: 0.8437\n",
      "Epoch [3/20], Loss: 0.7491\n",
      "Epoch [4/20], Loss: 0.6696\n",
      "Epoch [5/20], Loss: 0.5883\n",
      "Epoch [6/20], Loss: 0.5140\n",
      "Epoch [7/20], Loss: 0.4384\n",
      "Epoch [8/20], Loss: 0.3767\n",
      "Epoch [9/20], Loss: 0.3061\n",
      "Epoch [10/20], Loss: 0.2368\n",
      "Epoch [11/20], Loss: 0.1890\n",
      "Epoch [12/20], Loss: 0.1448\n",
      "Epoch [13/20], Loss: 0.1144\n",
      "Epoch [14/20], Loss: 0.1005\n",
      "Epoch [15/20], Loss: 0.0728\n",
      "Epoch [16/20], Loss: 0.1025\n",
      "Epoch [17/20], Loss: 0.0849\n",
      "Epoch [18/20], Loss: 0.0317\n",
      "Epoch [19/20], Loss: 0.0305\n",
      "Epoch [20/20], Loss: 0.0296\n",
      "Currently testing: a: 0.9, b: 0.09999999999999998, Prune: 1.0\n",
      "Original memory: 13.71 MB, Final memory: 6.00 MB\n",
      "Original num elements: 3594880, Final num elements: 50328320\n",
      "Using device: cpu\n",
      "Epoch [1/20], Loss: 1.0107\n",
      "Epoch [2/20], Loss: 0.8478\n",
      "Epoch [3/20], Loss: 0.7634\n",
      "Epoch [4/20], Loss: 0.6773\n",
      "Epoch [5/20], Loss: 0.6022\n",
      "Epoch [6/20], Loss: 0.5197\n",
      "Epoch [7/20], Loss: 0.4502\n",
      "Epoch [8/20], Loss: 0.3873\n",
      "Epoch [9/20], Loss: 0.3164\n",
      "Epoch [10/20], Loss: 0.2603\n",
      "Epoch [11/20], Loss: 0.2065\n",
      "Epoch [12/20], Loss: 0.1645\n",
      "Epoch [13/20], Loss: 0.1384\n",
      "Epoch [14/20], Loss: 0.0992\n",
      "Epoch [15/20], Loss: 0.0740\n",
      "Epoch [16/20], Loss: 0.0607\n",
      "Epoch [17/20], Loss: 0.0520\n",
      "Epoch [18/20], Loss: 0.0495\n",
      "Epoch [19/20], Loss: 0.0458\n",
      "Epoch [20/20], Loss: 0.0590\n",
      "a: 0.1, b: 0.9, Prune: None, Accuracy: 71.55%, Time: 11.75 seconds\n",
      "a: 0.1, b: 0.9, Prune: 0.6, Accuracy: 71.64%, Time: 10.18 seconds\n",
      "a: 0.1, b: 0.9, Prune: 0.8, Accuracy: 70.61%, Time: 10.55 seconds\n",
      "a: 0.1, b: 0.9, Prune: 1.0, Accuracy: 70.98%, Time: 8.67 seconds\n",
      "a: 0.3, b: 0.7, Prune: None, Accuracy: 71.30%, Time: 11.26 seconds\n",
      "a: 0.3, b: 0.7, Prune: 0.6, Accuracy: 69.57%, Time: 10.85 seconds\n",
      "a: 0.3, b: 0.7, Prune: 0.8, Accuracy: 70.84%, Time: 10.71 seconds\n",
      "a: 0.3, b: 0.7, Prune: 1.0, Accuracy: 69.50%, Time: 8.54 seconds\n",
      "a: 0.5, b: 0.5, Prune: None, Accuracy: 72.94%, Time: 11.08 seconds\n",
      "a: 0.5, b: 0.5, Prune: 0.6, Accuracy: 71.55%, Time: 10.72 seconds\n",
      "a: 0.5, b: 0.5, Prune: 0.8, Accuracy: 70.55%, Time: 10.85 seconds\n",
      "a: 0.5, b: 0.5, Prune: 1.0, Accuracy: 69.82%, Time: 9.29 seconds\n",
      "a: 0.7, b: 0.30000000000000004, Prune: None, Accuracy: 72.53%, Time: 11.23 seconds\n",
      "a: 0.7, b: 0.30000000000000004, Prune: 0.6, Accuracy: 72.19%, Time: 9.92 seconds\n",
      "a: 0.7, b: 0.30000000000000004, Prune: 0.8, Accuracy: 72.39%, Time: 10.25 seconds\n",
      "a: 0.7, b: 0.30000000000000004, Prune: 1.0, Accuracy: 71.11%, Time: 9.50 seconds\n",
      "a: 0.9, b: 0.09999999999999998, Prune: None, Accuracy: 72.67%, Time: 11.77 seconds\n",
      "a: 0.9, b: 0.09999999999999998, Prune: 0.6, Accuracy: 72.16%, Time: 11.35 seconds\n",
      "a: 0.9, b: 0.09999999999999998, Prune: 0.8, Accuracy: 72.00%, Time: 10.12 seconds\n",
      "a: 0.9, b: 0.09999999999999998, Prune: 1.0, Accuracy: 71.57%, Time: 9.60 seconds\n"
     ]
    }
   ],
   "source": [
    "# a and b values and prune param\n",
    "a_list = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "prune_list = [None, 0.6, 0.8, 1.0]\n",
    "acc_list = []\n",
    "time_list = []\n",
    "\n",
    "for a in a_list:\n",
    "  for prune in prune_list:\n",
    "    print(f\"Currently testing: a: {a}, b: {1-a}, Prune: {prune}\")\n",
    "    baseline_hyperparameters[\"a\"] = a\n",
    "    baseline_hyperparameters[\"b\"] = 1-a\n",
    "    baseline_hyperparameters[\"prune_param\"] = prune\n",
    "    acc, time_taken = get_results(baseline_hyperparameters, system_params)\n",
    "    acc_list.append(acc)\n",
    "    time_list.append(time_taken)\n",
    "\n",
    "idx = 0\n",
    "for a in a_list:\n",
    "  for prune in prune_list:\n",
    "    print(f\"a: {a}, b: {1-a}, Prune: {prune}, Accuracy: {acc_list[idx]:.2f}%, Time: {time_list[idx]:.2f} seconds\")\n",
    "    idx += 1\n",
    "\n",
    "baseline_hyperparameters = baseline_hyperparameters_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
